[{"content":"Data Artifacts An anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive 1. These anti-patterns can be identified in management approaches, software design, programming, and probably whenever people come together to achieve anything.\nData Science has its own set of anti-patterns that data scientist and their project managers should be aware of. “Proliferating Data Artifacts” refers to the behaviour of generating datasets which represent various stages of processing. The data scientists often have to wrangling the data into the right shape for analysis. This might include basic things like joining, filtering, mapping and aggregating data as well as any number of complex feature engineering techniques. There is a strong incentive to save the processed data after each processing step to avoid unnecessary re-processing when changes to the code affect only later steps.\nThe choice to save intermediate data artifacts can, however, become very costly when multiple versions of the same data exist. We all know this from trying to save word documents as _v1.doc, _v2.doc, etc. A colleague might email back some changes, but renames the document to _v2_John.doc destroying the whole idea of a well-defined genealogy of documents. God forbid anyone ever tries to save a document as _final.doc. Future versions with an ever-growing _final_final_... suffix are all but inevitable.\nUnlike text documents, datasets can easily take up gigabytes of space. Multiplied with the number of distinct processing steps and their variations this can grow rapidly to a level where the dreaded \u0026lsquo;out-of-space\u0026rsquo; warning hits the poor data scientist. According to Murphy\u0026rsquo;s law this will always happen just before a critical deadline. In any case, the data scientist often finds it hard to decide which dataset to delete, as he/she might be unsure which one was the latest or if it is still needed. As a consequence a disproportionate amount of time is spent on managing intermediate data artifacts and storage space.\n The \u0026ldquo;Proliferating Data Artifacts\u0026rdquo; anti-pattern leads to the unnecessary management of intermediate data artifacts and storage space.\n Pipelines to the Rescue But as with every anti-pattern — there is a solution! Pipelines. Pipelines manage intermediate datasets under-the-hood. A pipeline is basically a directed acyclic graph that describes the step-wise processing of the data.\nIt is easy to see that any data artifact could be recreated by applying all the processing steps (arrows) that lead to it to the preceding data. All that is needed is the input data and a well-defined pipeline.\nThe pipeline approach has several advantages:\n The user does not need to track, save, or pass on (large) data artifacts. Unnecessary re-processing of unchanged/unaffected data artifacts is avoided. A pipeline is defined in code and can be versioned. Data artifacts from any pipeline version can easily and reproducibly be re-created.  There are many more advantages that a good pipeline implementation can offer, such as parallel processing of independent parts of the pipeline, being self-contained, restart after failure, and efficient propagation of data changes.\nSo there\u0026rsquo;s really no excuse not to use existing pipeline frameworks (or implement them yourself) and delete those proliferating data artifacts! 2\n  Wikipedia: Anti-Pattern\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The original version of this text was published 2018-05-09.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://jan-scholz.github.io/post/2018-03-09-data-science-anti-patterns/","summary":"Data Artifacts An anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive 1. These anti-patterns can be identified in management approaches, software design, programming, and probably whenever people come together to achieve anything.\nData Science has its own set of anti-patterns that data scientist and their project managers should be aware of. “Proliferating Data Artifacts” refers to the behaviour of generating datasets which represent various stages of processing.","title":"Data Science Anti-Patterns: Proliferating Data Artifacts"},{"content":"1. Serve your model quickly. The ultimate measure of success is that the insights generated by your model are useful to someone. The sooner that someone looks at the model output the sooner he/she can provide valuable feedback. I\u0026rsquo;d not be surprised if currently the vast majority of advanced analytics projects are shelved or abandoned, because the predictions cannot be integrated into business processes in a meaningful way. Or something is predicted that was already known by other means. A quickly-served model will also help you answer all your questions around performance, visualization, and production environment. As a result you\u0026rsquo;re less likely to optimize prematurely, over-deliver, and you can ensure that the model can continue to exist past the end of the project.\n2. Data Science projects are inherently iterative. Data quality can usually not be determined before modelling and model experimentation might affect data acquisition and pre-processing. Going through these iterations is often more efficient than trying to ascertain truths about data quality and model choice in advance. Many clients are worried about the quality and quantity of their data and might want to delay the start of a data science project until they\u0026rsquo;re sure that success is guaranteed. However, the most efficient way to determine data quality is by modelling.\n Data science projects are people projects.\n 3. Data Science projects are similar to software projects. They benefit from team spirit, improved communication, and a culture that encourages experimentation and learning. Clients are often concerned that they\u0026rsquo;re not getting the ‘best’ model and the ‘most accurate’ prediction. But the fact that the speed and quality of delivery could be easily improved with a few simple measures is often forgotten. In the end, data science projects are people projects. Things like a distraction-free work environment, easy access to an adequate development environment, ergonomic workplace and hardware, and easy access to data, have a bigger impact on success than a particular model choice.\n Bonus Rule: Chose technology and tooling to support all of the above.\n Three quick rules for successful Data Science projects. Interested readers might want to learn about a structured Data Science approach like CRISP-DM 1 next. 2\n  CRoss Industry Standard Process for Data Mining (CRISP-DM)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The original version of this text was published 2018-02-08.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://jan-scholz.github.io/post/2018-02-08-three-rules-of-success/","summary":"1. Serve your model quickly. The ultimate measure of success is that the insights generated by your model are useful to someone. The sooner that someone looks at the model output the sooner he/she can provide valuable feedback. I\u0026rsquo;d not be surprised if currently the vast majority of advanced analytics projects are shelved or abandoned, because the predictions cannot be integrated into business processes in a meaningful way. Or something is predicted that was already known by other means.","title":"Three Rules of Success for Data Science Projects"}]