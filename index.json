[{"content":"It was an absolute pleasure to be invited to present at the Machine Learning Toronto Meetup.\nHuge shout-out to Myles of NLP from Scratch-fame and the other organizers for hosting such an amazing meetup. Abhi delivered a breathtaking deep dive into optimizing information retrieval pipelines down to the CPU level.\nThat was a tough act to follow with my \u0026ldquo;Journey to develop user-friendly applications that provide insights at enterprise scale\u0026rdquo;. But I was glad to see it sparked some interesting conversations around how to make time for teams to learn and how to practice domain-driven design. Here are my slides.\n","permalink":"https://www.talesofindustry.org/post/2024-09-30-mlto-talk-ai-product-development/","summary":"\u003cp\u003eIt was an absolute pleasure to be invited to present at the Machine Learning Toronto Meetup.\u003c/p\u003e\n\u003cp\u003eHuge shout-out to \u003ca href=\"https://www.linkedin.com/in/mylesharrison/\"\u003eMyles\u003c/a\u003e of \u003ca href=\"https://www.nlpfromscratch.com\"\u003eNLP from Scratch\u003c/a\u003e-fame and the other organizers for hosting such an amazing \u003ca href=\"https://www.meetup.com/machine-learning-to-meetup/events/302696444/\"\u003emeetup\u003c/a\u003e. \u003ca href=\"https://www.linkedin.com/in/abhimanyu-anand/\"\u003eAbhi\u003c/a\u003e delivered a breathtaking deep dive into optimizing information retrieval pipelines down to the CPU level.\u003c/p\u003e\n\u003cp\u003eThat was a tough act to follow with my \u0026ldquo;Journey to develop user-friendly applications that provide insights at enterprise scale\u0026rdquo;. But I was glad to see it sparked some interesting conversations around how to make time for teams to learn and how to practice domain-driven design. Here are my \u003ca href=\"/docs/Tales%20of%20Industry%20-%20AI%20Product%20Development.pdf\"\u003eslides\u003c/a\u003e.\u003c/p\u003e","title":"MLTO Talk: What I learned about AI Product Development"},{"content":"Introduction The tech industry is far from being as diverse as it should be. Women make up around 23-25% of the US \u0026ldquo;tech workforce\u0026rdquo; in major tech companies1. These numbers are well below the total participation rate of women in these companies (29-45%). The situation is even worse for the field of data science, where \u0026ldquo;of all the various tech fields, data science currently ranks the lowest in diversity\u0026rdquo;2. It\u0026rsquo;s not difficult to imagine that things are not much better for data science teams in large enterprises such as banks, telcos and retailers. With all the caveats associated with online surveys, the data suggests that only about 20% of data scientists across industries are women3.\nIf organizations reduce diversity to a matter of meeting quotas, they fail to understand that diverse teams are essential to solving complex challenges like AI.\nThese numbers paint a dire picture of the AI industry without even considering other kinds of diversity. In addition to the important question of equity, diverse teams are also more innovative and adapt more easily to a rapidly changing environment4. A quota-focused approach that disregards the inherent value of teams with varied backgrounds fails individuals and organizations alike. In the following, I will look at diversity through three distinct lenses, aiming to unveil a more nuanced perspective that can help organizations find a better approach to diversity.\nNote, I\u0026rsquo;m not arguing that one kind of diversity is more important than another. Organizations should strive to address all of the following and more.\n1. Gender and Cultural Diversity Gender and culture dominate the diversity discussion. Plans to increase diversity are often measured by how they address the gender ratio and the cultural (including ethnic, racial, and religious) markup of the employee base. Overall, the tech industry has made huge efforts to attract more women, LGBTQ+ members, and people of colour by creating better training opportunities, adjusting their talent search, and making the workplace more inclusive5.\nVarious theories, such as the \u0026ldquo;leaky pipeline theory\u0026rdquo; and the \u0026ldquo;hostile workplace theory\u0026rdquo; have been brought forward to explain the persistent gender gap6. And structural barriers keep certain minorities from participating according to their labor participation rate. Meta, Google, Apple and others are backing affirmative action7 to increase diversity in higher education and thus their talent pools.\nGender and cultural diversity are the most visible markers of diversity as they can be communicated as part of one\u0026rsquo;s identity. A lack of diversity along these dimensions is thus more apparent, making gender and ethnicity targets for diversity quotas. However, organizations that focus on a few visible markers of diversity might get stuck at a very shallow understanding of the \u0026ldquo;why\u0026rdquo;.\nThis is ostensibly the case when increasing diversity is seen as a cost to the business. Too many organizations rely on employees to \u0026ldquo;volunteer\u0026rdquo; and organize events with the occasional external diversity representative being invited as a guest speaker. Unfortunately, this can mislead members of these organizations into thinking that diversity is meaningfully addressed, because these events happen.\nIt becomes quickly apparent how interested an organization is in promoting diversity, when actual change and investment are required. Is money spent to find and recruit women, LGBTQ+ members, and people of colour? Or does the organization put the burden on candidates to present themselves that way on LinkedIn? Are external communities and training organizations supported to help increase the talent pool for the whole industry? How about accessibility, accommodations, and promotions?\nAll of the above requires meaningful financial and organizational commitment. Deepening the understanding of what diversity really means can help avoid seeing it as a prescribed target that has to be achieved in the most cost-effective manner.\n2. Cognitive \u0026amp; Behavioural Diversity A diversity approach that is monomaniacally focused on achieving quotas misses additional dimensions of diversity. While gender and cultural diversity have been shown to be beneficial for organizations, this has been attributed to the fact that teams with people who think differently and do things differently are more creative at problem solving. A team or organization with low cognitive diversity can suffer from \u0026ldquo;groupthink\u0026rdquo; which leads to poor decision making and the failure to address risks. Put differently:\nHomogeneous teams produce homogeneous outcomes8.\nTaking cognitive diversity into account presents another opportunity for organizations to push themselves to create an overall better and more accommodating workplace. For example, individuals who tend to be more introverted might get overlooked in organizations which have a culture of dominance. These organizations deprive themselves of the value that introverted individuals can contribute, while at the same time failing to provide an equitable workplace.\nCognitive and behavioural diversities refer to the different ways that individuals perceive the world, learn, make decisions, communicate, and act. Some of these characteristics have been popularized by the various (pseudoscientific) personality tests that large organizations and consultancies are so enamoured with. Independently of what the benefits of particular cognitive and behavioural traits are, it should be apparent that there is often no \u0026ldquo;best way\u0026rdquo; of doing things when it comes to solving complex, novel problems such as AI. Thus, teams that can explore multiple approaches have a better chance of discovering solutions.\nCognitive diversity is important at all levels of hierarchy. It helps improve decision making, increases adaptability in a fast changing environment, enhances strategic anticipation, and promotes innovation4; all of which are vital for AI adoption.\n3. Diversity of Experience The different kinds of diversity described above are obviously not mutually exclusive. Gender and cultural diversity are probably correlated with cognitive diversity due to differences in upbringing, schooling, and social expectations. It\u0026rsquo;s thus not wrong in itself for an organization to address more visible markers of diversity.\nHowever, focusing on a few select markers of diversity can distract from the more expansive definition of what I will call \u0026ldquo;diversity of experience\u0026rdquo; for short. Ultimately, an organization (as an entity with a purpose) should have a self-interest in being accommodating to individuals with a wide range of experiences to avoid getting trapped in the thinking and decision making of a particular group.\nThe fact that we are still far away from achieving diversity goals \u0026ndash; even in the AI industry that would benefit the most \u0026ndash; suggests that Goodhart\u0026rsquo;s Law9 is still alive and strong. Quotas and metrics can help an organization steer in the right direction, but they come with the risk of simplifying things and overlooking the depth and complexity of the challenge. Metrics will be \u0026ldquo;gamed\u0026rdquo; and perceived as another box to tick with the least amount of effort possible.\nA wholesome appreciation of the value that \u0026ldquo;diversity of experience\u0026rdquo; brings to a team could help all members of an organization unite around a common goal of increasing diversity.\nConclusion Unfortunately, the tech sector \u0026ndash; and the AI industry in particular \u0026ndash; are still less diverse than the general labour force. This is surprising, because diverse teams are essential to solving complex challenges like AI.\nEquity is often presented in a language of social discourse that enterprises find challenging to integrate into their hiring and workplace processes. They live in a world of key performance metrics, quotas, budgets, and cost optimization.\nI tried to outline a complementary framework that could serve as a counter-narrative to the prevailing surface-level understanding of diversity. What if diversity is desirable in and of itself for an organization, because it has value? It should thus be in every organization\u0026rsquo;s self-interest to provide a workplace that is equitable and attracts diversity.\nWomen\u0026rsquo;s Representation in Big Tech\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCrunching the Numbers on Diversity in Data Science: Events \u0026amp; Resources to Foster Inclusion\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow are the 💃Ladies and the 🎩Gents doing?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeams Solve Problems Faster When They’re More Cognitively Diverse\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDiversity in Data Science: A Systemic Inequality\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGender Diversity in the Tech Industry — What does the literature say?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBig Tech lends its support in Harvard affirmative action case\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow to enable creativity and innovation: Work with diverse teams in an inclusive culture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoodhart\u0026rsquo;s law is an adage often stated as, \u0026ldquo;When a measure becomes a target, it ceases to be a good measure\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2024-02-10-three-kinds-of-diversity/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThe tech industry is far from being as diverse as it should be. Women make up around 23-25% of the US \u0026ldquo;tech workforce\u0026rdquo; in major tech companies\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. These numbers are well below the total participation rate of women in these companies (29-45%). The situation is even worse for the field of data science, where \u0026ldquo;of all the various tech fields, data science currently ranks the lowest in diversity\u0026rdquo;\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e. It\u0026rsquo;s not difficult to imagine that things are not much better for data science teams in large enterprises such as banks, telcos and retailers. With all the caveats associated with online surveys, the data suggests that only about 20% of data scientists across industries are women\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"Three Kinds of Diversity"},{"content":"Introduction In this three-part series we discussed the challenges that enterprises face when it comes to adopting AI \u0026ndash; especially those organizations that have traditionally not been in the business of software engineering:\nIdentifying the right opportunities (Part I) Product development (Part II) Avoiding common pitfalls This final part will conclude the series by going over a few common pitfalls of AI product development and how to avoid them. This list is by no means exhaustive and focuses on business and organizational pitfalls1 rather than those related to AI theory and technology.\nAvoiding Common AI Pitfalls Get a business sponsor AI projects are by their very nature complex and show value only slowly. In this regard they are more akin to R\u0026amp;D or innovation-type projects that require an initial investment without immediate payback. It\u0026rsquo;s thus all the more important to secure strong support from the side of the business that is supposed to benefit from (and often funds) the work.\nA good proportion of data science teams struggle, because they have entered an antagonistic, one-sided relationship with the business where they are seen as service providers. As a result, these teams loose their ability to identify with the business\u0026rsquo; goals and start to look inward, focusing on technical challenges instead of driving business value.\nGetting a business sponsors who is invested into the team\u0026rsquo;s success can help mitigate these risks. A sponsor ensures that the business relationship is collaborative and that goals are shared across teams. Developing an effective relationship with business stakeholders can be difficult2 and data science teams can benefit from someone who can liaise between them and the stakeholders. Finally, a sponsor can also help support project funding discussions by communicating timelines and risks to decision makers.\nStart simple Once a business sponsor has been found it is time to define the scope of the project. Data scientists and ML engineers are an especially ambitious bunch \u0026ndash; which is great \u0026ndash; but often associated with a tendency to overengineer and overachieve. Especially at the start of a project it is expedient to scale back the aspirations of the team and focus on what value can be delivered quickly with a reasonable amount of effort.\nThis initial, imperfect solution can then be used as a springboard to get the buy-in to develop a more sophisticated product. The added benefit is that user feedback can be collected. The question of whether an AI product provides value to the user or customer carries the largest risk and can often be answered without taking the most sophisticated AI approach.\nIt takes discipline and experience to identify the business problems that have the right complexity and don\u0026rsquo;t require unreasonable time and effort before value is measurable. But a good guideline is to halve the complexity of the AI solution that was initially proposed. In many enterprises ample value can be delivered even after radically scaling back the scope, especially when this is the first attempt of rolling out AI or machine learning at scale for a particular business unit.\nStart with augmentation over automation Augmentation and human-in-the-loop solutions are a good example of simplifying AI product design during the initial roll-out. Although full automation might be the end goal, a transitional augmentation phase has many advantages.\nAugmentation can be an effective way to incrementally work towards automation. It often takes time to narrow down the exact problem to solve with AI. Individuals who used to complete a task manually can help capture all the implicit assumptions and business rules when they are part of the augmentation phase.\nAugmentation can also speed up time-to-market. An AI application can already start generating value by augmenting the human user on the easiest-to-automate tasks. The experience can demonstrate whether it is worth automating the entire workflow.\nSometimes augmentation can free up enough time3 for humans to focus on tasks where we\u0026rsquo;re still superior to machines. In the end, this approach can find the sweet spot between augmentation and automation, which would have been overlooked by attempting full automation from the get-go.\nOf course, augmentation can also help minimize risk where an error may compromise human safety or increase financial risk. The benefit of fully automating a task may not outweigh the cost of the AI making occasional mistakes.\nIn summary, aiming for augmentation first can help de-risk AI projects, especially in enterprises with low AI maturity.\nDon\u0026rsquo;t get distracted by data This final AI pitfall refers the lack of data access or availability that hobbles so many projects in the beginning. This is especially frustrating for data scientists who joined the project to work on machine learning models and want to get started as quickly as possible.\nWhile the reasons for a lack of data can be manifold, they often shift focus away from the business problem. Instead, time is spent deliberating with data management, data governance, or IT teams on how to gain access to already existing data. Or plans have to be made to collect or buy new data.\nAI products need to straddle data requirements, model insights and user experience. Focusing efforts on one corner of the triangle, e.g. data, can risk falling behind on modelling and user experience.\nThis is not to say that data isn\u0026rsquo;t important, but time spent on data availability is time not spent on working toward a great user experience. Data, model, and user experience are interdependent and cannot be solved independently. Spending time on data alone risks acquiring the wrong data or creating data infrastructure that is incompatible with the use case. Data lakes, which enterprises spend years filling, are a good example. AI product development is sometimes delayed by these projects and often requires significant adjustments to data and data infrastructure once started.\nIt should not be forgotten that building models and exposing them to the user can help inform the data acquisition process. Many of the questions around volume, state, and predictiveness of the data can only be answered by starting the modelling process. The question of whether the model’s insights are needed, useful, and actionable can only be answered once the user is exposed to them.\nConclusion Not every team or project will encounter the same AI product development pitfalls. Many problems can be avoided with experience, foresight, and adherence to best practices. But these pitfalls also stem in part from patterns that bias teams to behave in certain ways.\nTeams might be reluctant to interact with the business to get a sponsor, because these are usually people from a different background, who speak a different language. Teams might be ambitious, thinking they can or should solve the challenge by themselves. It\u0026rsquo;s easy to get enamoured with new, shiny frameworks and start designing solutions that are too complex and built on technology that people have little experience with.\nSo the journey to embrace best practices is also always a fight against the forces that pull us away from adopting them.\nRead about \u0026ldquo;how to identify AI opportunities\u0026rdquo; in Part I and \u0026ldquo;AI product development\u0026rdquo; in Part II of this series.\nHow AI Fits into Your Data Science Team\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow to Work With Stakeholders as a Data Scientist\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf done right, this will also provide a way to introduce AI more gracefully without threatening to displace someone\u0026rsquo;s job.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-07-30-ai-product-development-part3/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn this three-part series we discussed the challenges that enterprises face when it comes to adopting AI \u0026ndash; especially those organizations that have traditionally not been in the business of software engineering:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIdentifying the right opportunities (\u003ca href=\"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/\"\u003ePart I\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eProduct development (\u003ca href=\"https://www.talesofindustry.org/post/2022-06-01-ai-product-development-part2/\"\u003ePart II\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eAvoiding common pitfalls\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis final part will conclude the series by going over a few common pitfalls of AI product development and how to avoid them. This list is by no means exhaustive and focuses on business and organizational pitfalls\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e rather than those related to AI theory and technology.\u003c/p\u003e","title":"AI Product Development (Part III)"},{"content":"Introduction Enterprises face three broad challenges when it comes to adopting AI. These are especially relevant to organizations that have traditionally not been in the business of software engineering and AI:\nIdentifying the right opportunities (Part I) Product development Avoiding common pitfalls An approach to identifying the right opportunities \u0026ndash; the Double Diamond \u0026ndash; was presented in Part I of this three-part series. Here we will focus on the second challenge: Product development; or how to create an environment where successful AI products can be built confidently.\nAI Product Development Once the right problem has been identified, it\u0026rsquo;s time to build the solution. Easier said than done for organizations that have not been set up to engineer software products, let alone AI products. Even companies that are software engineering companies at heart, are not insulated from failure and have a long list of abandoned products. See the Google Graveyard, for example.\nBut AI product development does not have to be a source of frustration when a few principles are taken into account. These are well-understood in the software engineering industry. I\u0026rsquo;ll highlight a few of them below and how they apply to AI development specifically.\n1. People It\u0026rsquo;s alarming that even to this day some organizations try to \u0026ldquo;save\u0026rdquo; money by not adequately staffing what is essentially a software engineering project. Every little dev shop has by now figured out that building quality products requires a project manager, Scrum master, designers, developers, and QA engineers. AI products require additional resources, such as data engineers, data scientists, machine learning engineers, etc.\nA complete AI product team. Business stakeholders in ties. (Public Domain)\nAssuming that a team of data scientists is able to deliver robust software products continues to be the reason for many failed or massively delayed projects. It\u0026rsquo;s paramount to hire the roles required to build products, not just models.\nHigh-performance teams can do amazing work during the experimentation and proof-of-concept phase, but even they will reach their limit if you ask them to scale out a solution without the dedicated resources necessary to build and maintain robust cloud architecture.\nSome organizations find it challenging to attract this kind of talent. In those cases, start by investing in \u0026ldquo;seed\u0026rdquo; talent, i.e. people who have a network, perhaps in the start-up world. You should aim for at least 20-30% of highly skilled people in the early days of your organization\u0026rsquo;s AI transformation. These people can spearhead the changes necessary to adopt modern engineering practices and demonstrate how it\u0026rsquo;s done by example. Don\u0026rsquo;t forget to develop a talent strategy with your talent/HR team that allows you to attract and retain the right kind of talent.\nAll of this doesn\u0026rsquo;t happen overnight. It\u0026rsquo;s not unusual for organizations to make multiple attempts to build the right talent pool. Sometimes they oscillate between \u0026ldquo;all-in\u0026rdquo; (hire the A-Team) and \u0026ldquo;back-to-basics\u0026rdquo; (let IT do it) before they converge on a better understanding of their AI talent needs.\nSuffice to say, the talent an organization is able to attract and retain defines what can be achieved. An incomplete product team will build incomplete products.\n2. Process AI products are built in the context of an organization. Any development effort is reliant on the processes that govern the collaboration across teams and the access to the right tools and infrastructure.\nAgile and Scrum are widely know and will not be discussed here. However, there are two processes that are essential to AI product development and somewhat independent of agile: enablement via the right tools and discovery vs engineering.\nEnablement via the right tools Data scientists and machine learning engineers need the right tools to succeed. Nobody would equip a professional cycling team with city bikes, because that\u0026rsquo;s all they need to cross the finishing line.\nIt\u0026rsquo;s a sad fact of this industry that not all data science teams have access to adequate version control systems, developer-friendly computers, open source tools, live data or cloud infrastructure. These are very basic preconditions of success and an organization is undermining itself if talent is handicapped this way. It\u0026rsquo;s also disrespecting to not give someone the right tools to succeed at their job. Top talent will leave and an organization\u0026rsquo;s reputation as an attractive employer for AI talent will suffer.\nJan and Huub are trying out their new MacBooks. (Nationaal Archief)\nBeyond basic enablement, data science teams should use the tools that they can master and have a plan to learn the tools that are required. In practice that means staying away (at first) from frameworks that are overkill or beyond the team\u0026rsquo;s ability, like, for example, Kubernetes. On the flip side, teams that are working exclusively on Jupyter notebooks need to develop a plan to adopt the tools for building production-ready code, e.g. packaging and GitLab CI/CD.\nHaving a technology adoption strategy in a fast-moving ecosystem like AI is key. This includes giving the team the time to learn required tools and evaluate promising tools. The hiring process should be synchronized with the technology strategy to ensure that new hires have practical experience where needed and can possibly teach the team new tools. However, technology is there to enable teams and shouldn\u0026rsquo;t become a distraction or worse an end in itself.\nDiscovery vs Product Engineering Building a solution that derives insights from data requires two fundamental types of activities: discovery and product engineering1. The distinction is necessary, because the data often affects the solution design. Put differently, serious engineering cannot commence before the data (and therefore the opportunity) is sufficiently understood.\nLess AI-mature enterprises often get these phases mixed up and advances in discovery are sometimes viewed as product development progress. Business stakeholders can get frustrated when they are presented with discovery findings, but then have to wait a long time to see the same results in a fully scaled-out application. Similarly, data science teams can get caught up in an infinite discovery cycle where new findings beget further rounds of discovery.\nIt\u0026rsquo;s thus important to mitigate the risk of discovery by properly scoping and time-boxing the activity with the goal of flowing the findings into the final product. At the same time, any engineering activity should not be rushed until there\u0026rsquo;s evidence in the data that the activity if valuable.\nFor example, building a forecasting engine requires testing the hypothesis that a prediction of sufficient accuracy can be made. The forecasting accuracy can be evaluated during the discovery phase without investing into a scaled out forecasting engine. The next step is then to eliminate the risk of technical feasibility, i.e. can the forecasting be run across all data at the desired frequency. This phase should be focused on the implementation and not be derailed by further feature enhancements to the model. The model\u0026rsquo;s feature enhancements can then be addressed in the next round of discovery and product engineering.\nThere are frameworks, such as CRISP-DM2, that formalize the transition between different phases during the iterative development of AI products. It\u0026rsquo;s helpful to adopt a process that facilitates the flow of discovery findings into the AI product to establish a healthy cadence of product advancement.\nConclusion The right team and processes are instrumental to an organization\u0026rsquo;s success when it comes to building AI products. Although most teams have to operate under resource constraints, it\u0026rsquo;s still worthwhile to develop a talent and a technology enablement strategy. These strategies make sure that the available resources are invested where they matter most.\nWe will address the final challenge to AI adoption (\u0026ldquo;avoiding common pitfalls\u0026rdquo;) in the upcoming Part III of this series. Read about \u0026ldquo;how to identify AI opportunities\u0026rdquo; in Part I.\nData Science vs Software Engineering\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCRoss Industry Standard Process for Data Mining (CRISP-DM).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-06-01-ai-product-development-part2/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eEnterprises face three broad challenges when it comes to adopting AI. These are especially relevant to organizations that have traditionally not been in the business of software engineering and AI:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIdentifying the right opportunities (\u003ca href=\"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/\"\u003ePart I\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eProduct development\u003c/li\u003e\n\u003cli\u003eAvoiding common pitfalls\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn approach to identifying the right opportunities \u0026ndash; the Double Diamond \u0026ndash; was presented in \u003ca href=\"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/\"\u003ePart I\u003c/a\u003e of this three-part series. Here we will focus on the second challenge: \u003cem\u003eProduct development\u003c/em\u003e; or how to create an environment where successful AI products can be built confidently.\u003c/p\u003e","title":"AI Product Development (Part II)"},{"content":"Introduction Depending on how you count, \u0026ldquo;legacy\u0026rdquo; enterprises have tried to adopt modern machine learning for close to a decade. And while some organizations have been successful, there are still large swaths of companies that are fighting to even automate their internal reporting. At the same time, data science training is widely available and lots of machine learning tools have matured and are freely available as open source. So why have some organizations barely started to leverage AI?\nNon-digitally native enterprises often struggle with three broad challenges when it comes to creating AI-powered products.\nIdentifying the right opportunities Product development Avoiding common pitfalls Identifying AI Opportunities The news are awash with the most extraordinary advances in AI and machine learning. AI can write entire essays to answer questions1, generate convincing portraits2, and create pictures according to a description3.\nIronically, all that does is to set the expectations for AI much too high for your typical enterprise. Companies such as Amazon, Google, Microsoft, Apple, and Meta spend tens of billions of dollars on R\u0026amp;D4. Don\u0026rsquo;t expect the same outcome with any less commitment!\nIn any case, identifying AI opportunities in an enterprise context is often approached backwards. Starting with a particular ML approach (no matter how exciting) and then trying to find problems that can be solved by it, is a sure-fire way to solving the wrong problems.\nEnterprises need to follow a human-centered approach that starts with understanding user needs and see AI as a tool, a very powerful one, that allows us to imagine further than we ever thought possible.\nThe Double Diamond Framework The Double Diamond Framework puts the observation and discovery of user needs at the start of the problem-solving process. There are various alternative Design Thinking approaches, all of which try to help answer the two most important questions of product ideation and development.\n1. What is the right _problem_ to solve? 2. What is the right _way_ to solve it? These two questions correspond to the two diamonds of the Double Diamond Framework. The goal of the first diamond is to ensure that we are solving the right problem. We do this by uncovering the core challenges faced by users and translating these insights into ideas during the divergent discovery phase. All ideas are then filtered to ensure that they are desirable and feasible within the project context during the convergent definition phase. Once the problem has been well defined, we move to the second diamond.\nThe Double Diamond highlights the two stages of Design Thinking and the alternation between divergent and convergent thinking.\nThe goal of the second diamond is to solve the problem the right way. We do this by designing and building the experience often in form of a prototype or MVP during the development phase. Any assumptions can be verified quickly based on the data gathered via a solid build-measure-learn process. The final delivery phase is then all about delivering a viable solution to the user or customer.\nThe Double Diamond framework has been described in more detail here5 and here6. It might, however, be worth highlighting how it applies to AI specifically.\nThe table summarizes some key activities and outcomes of Double Diamond\u0026rsquo;s four phases. These might look familiar to anyone who has used this approach before to build a software product. The \u0026ldquo;AI Awareness\u0026rdquo; column highlights the questions that can be asked by the product designer and members of the AI team to identify AI opportunities. They should be asked with humility and not with the aim to shoehorn AI into the final solution.\nStep Activities Outcomes AI Awareness 1. Explore and Learn User interviews Existing experience journey map Synthesize and prioritize User needs across social, emotional and functional dimensions Pain points across the end-to-end user journey Is the task repetitive? Is work offshored/ outsourced? Is the quality of work variable? 2. Ideate and Define Future state journey map \u0026amp; ideation Low fidelity designs Product vision, objectives, and measures of success Product roadmap What if we could automate X? What if we could support task X with predictions and insights? 3. Design and Develop High fidelity design Prototyping \u0026amp; technical implementation Build and launching a product or feature Consider starting with few/ simulated data Test model output with users early 4. Evaluate and Evolve Measure user behaviour Scale \u0026amp; and stabilize Validate hypothesis Determine whether pivot or persevere Continue to learn from user feedback Monitor model performance \u0026amp; address model drift \u0026copy; 2022 talesofindustry.org Being aware of the tremendous capabilities that AI provides can lead to better ideas. For example, we can consider AI whenever we come across users who are frustrated by having to engage in tedious or repetitive tasks. AI is well-suited for automating tasks that require insights derived from unstructured data, such as images and text.\nAgain, AI should not be the starting point. The design process starts with the user and their frustrations, and then devise ideas to solve the problem. These ideas might or might not include AI as a tool to engineer the better solution.\nConclusion A formalized process, such as the Double Diamond, has been proven useful across the industry and can be adapted to include AI as discussed. If your organization is struggling to identify opportunities and deliver on them, you can try to adopt this approach. Depending on the maturity level of your organization or team, it might also be worthwhile hiring a specialist who can provide training and advice. There\u0026rsquo;s no need to \u0026ldquo;reinvent the wheel\u0026rdquo;. Taking a human-centered approach by starting with user needs and then thinking about how AI can address those needs is a powerful recipe for building experiences users love7.\nWe will address the two remaining challenges to AI adoption (\u0026ldquo;product development\u0026rdquo; and \u0026ldquo;avoiding common pitfalls\u0026rdquo;) in the upcoming Part II of this series.\nGPT-3 Powers the Next Generation of Apps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlias-Free Generative Adversarial Networks (StyleGAN3)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDALL·E: Creating Images from Text\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoogle Needs To Make Machine Learning Their Growth Fuel\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhat is the framework for innovation?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDesign Thinking 101 — What Is It? (Part I), (Part II)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis article is based on a version Aneesh Datta and I previously published on Rangle.IO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eDepending on how you count, \u0026ldquo;legacy\u0026rdquo; enterprises have tried to adopt modern machine learning for close to a decade. And while some organizations have been successful, there are still large swaths of companies that are fighting to even automate their internal reporting. At the same time, data science training is widely available and lots of machine learning tools have matured and are freely available as open source. So why have some organizations barely started to leverage AI?\u003c/p\u003e","title":"AI Product Development (Part I)"},{"content":"Getting Started with Hugo Hugo is a static site generator (SSG). Simply put, an SSG prepares everything needed to display a website. In contrast, dynamically assembled websites obtain their assets only after they get accessed. An SSG thus minimizes the computational demand on the server, which allows providers, such as GitHub Pages and others1, to host your static sites for free. More importantly, SSGs emanate a philosophy of simplicity-by-design stemming from a number of limitations. Being constraint forces content creators to focus on the content itself and minimizes the distractions of layout and design. SSGs are thus especially suited for blogs, landing pages, documentation and portfolio sites.\nThe following is based on Ryan Schachte\u0026rsquo;s excellent video2 and similar tutorials (see links below).\nInstall Hugo On MacOS install Hugo from the command line using the Homebrew package manager.\nbrew install hugo On Debian Linux install Hugo with:\nsudo apt-get install hugo More installation instructions can be found here.\nSetting Up Two Repositories We will use GitHub Pages to host our static site for free. GitHub Pages are easily set up by creating a new public repository named username.github.io, where username is your GitHub username. If you were do save a simple HTML page as index.html in such a repo, you\u0026rsquo;d be able to view it at https://username.github.io.\nHowever, we\u0026rsquo;ll be slightly more considerate than dumping all of Hugo\u0026rsquo;s configuration code, templates, themes and markdown content into the GitHub repository. After all, to display the website correctly only the static site assets that Hugo compiles are needed. We are going to track all the rest in a separate repository called \u0026ldquo;blog\u0026rdquo;.\nSo please head over to GitHub and create two repositories:\nusername.github.io: A public repository for the static site assets. blog (can have a different name): A private repository for Hugo containing configurations, themes, and unpublished posts. Setting up Hugo Once we\u0026rsquo;ve created two repositories, we\u0026rsquo;ll clone them on our local system (git clone git@github.com:username/blog.git) and enter the \u0026ldquo;blog\u0026rdquo; repository (cd blog).\nHere we create the scaffolding for our website. In this case Hugo will create a new directory hugo-blog that contains all the Hugo assets.\nhugo new site hugo-blog Adding a Theme Enter the hugo-blog/themes directory and clone your preferred Hugo theme. For example, Hugo Bear Blog from Jan Raasch. Click on the \u0026ldquo;Download\u0026rdquo; button, copy the repository URL, and clone the theme into your themes/ folder using the submodule action. The submodule action avoids problems arising from nested cloning3.\n# Option 1: Original theme repo git submodule add https://github.com/janraasch/hugo-bearblog.git themes/hugo-bearblog Alternatively, you can first fork the theme directory by going to GitHub and clicking \u0026lsquo;Fork\u0026rsquo;. This has the advantage of being able to push changes to your fork of the themes directory later on.\nDon\u0026rsquo;t forget to add the fork as a submodule instead of the original repo. Replace username and hugo-theme with the respective values.\n# Option 2: Forked theme repo git submodule add https://github.com/username/hugo-theme.git themes/hugo-theme Then we add the theme\u0026rsquo;s name to the config.toml in blog\u0026rsquo;s base directory, e.g. hugo-blog/. The file might look like this:\nbaseURL = \u0026#39;https://username.github.io/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My Blog Title\u0026#39; theme = \u0026#39;hugo-bearblog\u0026#39; The values of baseURL and theme obviously depend on your choice of host and theme.\nAdding the static site content repo as a submodule As mentioned above, we are decided to create a separate repository for the compiled static site content. In essence, we will let the username.github.io repository only track Hugo\u0026rsquo;s public/ output directory and nothing else. This way the commit history will purely reflect content updates. Plus, we can track any unpublished content in a private repo hidden from public view. The downside is that we have to remember to update both repositories.\nRun the following command from within the blog\u0026rsquo;s base directory (e.g. hugo-blog/) to link your public GitHub Pages repo to the public/ directory of your private blog repo. (If you receive an \u0026lsquo;already exists\u0026rsquo; error you might have to remove the public/ directory or its contents first.)\ngit submodule add -b main https://github.com/username/username.github.io.git public Building the site To perform the site build, run the following commands\n# create static assets in the \u0026#39;public/\u0026#39; directory hugo # commit the changes to the public submodule repository cd public git add . git commit -m \u0026#34;initial build\u0026#34; # commit references to submodule changes cd .. git add . git commit -m \u0026#34;initial build - update submodule references\u0026#34; # push both the source project and the submodule to remote git push -u origin main --recurse-submodules=on-demand Remember these command, you\u0026rsquo;ll have to repeat them for most changes to the site.\nThis concludes the basic Hugo setup. In the following we\u0026rsquo;ll go through posting, theme customization, adding images, and analytics. Let\u0026rsquo;s create our first post.\nCreating a Post Hugo posts are created in Markdown. The limited formatting options force authors to focus on the content. In general, it\u0026rsquo;s good advice not to try to change the formatting beyond what markdown offers. The theme\u0026rsquo;s template and CSS files provide better options while ensuring that the layout stays consistent across the site.\nTo create a new post, run:\nhugo new blog/mypost.md This will create a markdown file in the content/ directory, e.g. content/blog/mypost.md. Depending on your theme, you might have to substitute blog/ with post/ or posts/. Consult your theme\u0026rsquo;s documentation.\nOpen the freshly created post with your favorite editor. Hugo posts start with a preamble, the key: value pairs between the lines (---). The preamble sets various variables that Hugo will use to compile the site. Depending on the theme, you can add cover images, tags, and category labels. The draft status and publication date affect whether a post is published. You can add content below the preamble, like so.\n--- title: \u0026#34;My First Post\u0026#34; date: 2020-01-01T12:15:00+01:00 draft: true --- # Introduction Some **bold** text and a [link](example.com). Now fire up the Hugo server locally in \u0026lsquo;draft\u0026rsquo; mode and open the respective URL with your browser, e.g. http://localhost:1313.\nhugo server -D If you\u0026rsquo;re new to Hugo, check out a particular theme\u0026rsquo;s example content. Not every theme will display posts the way you might expect. If you get stuck, start with the theme\u0026rsquo;s configuration files (config.toml) and content directories. The Bearblog example site is a good start if you decided to use this theme.\nDrafts and Future Posts Note that the server doesn\u0026rsquo;t show blog posts with draft status by default. You can either set draft: true in the markdown file\u0026rsquo;s preamble or run the hugo server in draft mode. Besides draft status there are two more conditions that might prevent a post from being published, i.e. posts with a publication date in the future and posts with an expiry date4.\n# compile posts with draft status hugo server -D # compile posts with a future publication date hugo server -F Publishing To publish your site, run Hugo excluding drafts and future posts and push the changes to the remote repositories as described above.\nTracking Visitors Hugo and various themes make it easy to add analytics. Start by creating a new account or sign in with an existing Google account at https://analytics.google.com/.\nThen set up your \u0026ldquo;Property\u0026rdquo;, give it a name, and point it to the URL of the site you plan on tracking. Finally, click through the basic options until you land on a page with a Tracking Code which might look something like this: G-XXXXXXXXXX.\nEdit Hugo\u0026rsquo;s configuration file, e.g. config.toml, and add the tracking code.\ngoogleAnalytics = \u0026#39;G-XXXXXXXXXX\u0026#39; You should now be able to track visitors after publishing these changes. Then click on \u0026lsquo;Reports\u0026rsquo; and then on \u0026lsquo;Realtime\u0026rsquo; on https://analytics.google.com/ to track yourself visiting the page. This should work even if you run a server on your own machine and browse the site locally.\nCustomization Hugo provides endless possibilities for customization. Virtually everything can be adjusted, including spacing, font sizes, colors, text alignment, and content interaction. The basic principle is to \u0026lsquo;override\u0026rsquo; a theme\u0026rsquo;s default templates. It\u0026rsquo;s relatively easy to use a browser\u0026rsquo;s \u0026lsquo;inspection\u0026rsquo; tools, click on the ⌖ , and hover over the element that you want to adjust. This should help you understand what CSS properties are being applied to this element. You might also be able to glean a few keywords that you can later use to search for.\nModifying a Theme Most theme modifications can be done without modifying the theme\u0026rsquo;s source code repository. Hugo allows overwriting template files with your own modified version that you keep in a separate directory5.\nFor example, it\u0026rsquo;s relatively straight-forward to modify the copyright footer because most themes keep its template in a separate file. Let\u0026rsquo;s assume the footer template is kept in\nthemes/\u0026lt;THEME\u0026gt;/layouts/\u0026lt;SUBDIR\u0026gt;/footer.html Obviously, the \u0026lt;THEME\u0026gt; and \u0026lt;SUBDIR\u0026gt; directories depend on your particular theme. If you have trouble finding the right file, browse your theme\u0026rsquo;s layouts/_default directory or grep for a keyword, e.g. grep footer -R. That should give you a rough idea of where to look.\nOnce we have identified the right file we make a copy of it and place it in:\nlayouts/\u0026lt;SUBDIR\u0026gt;/footer.html You might have to create the directory structure first, e.g. mkdir -p layouts/\u0026lt;SUBDIR\u0026gt; before you copy the template with cp -iv themes/\u0026lt;THEME\u0026gt;/layouts/\u0026lt;SUBDIR\u0026gt;/footer.html layouts/\u0026lt;SUBDIR\u0026gt;/\nThis copy has precedence over the theme\u0026rsquo;s \u0026lsquo;default\u0026rsquo; templates. You can edit the copy to your heart\u0026rsquo;s content. The changes should be immediately visible (or any errors) if you\u0026rsquo;re running the server (hugo server) at the same time. Reverting back to the original theme is as easy as deleting the file.\nOverride CSS To override the theme\u0026rsquo;s style sheet, consult its documentation. Most themes support placing one or several CSS files in the assets directory.\nassets/css/extended/custom.css Images Images help your site stand out. They are added to the static/ directory or any subdirectory within. For example, after creating a subdirectory (mkdir -p static/images/), images can then be referenced in markdown.\n![Alt Description](/images/cover.jpeg) Alternatively, some themes allow adding a caption (i.e. title) by using the Go syntax.\n{{\u0026lt; figure src=\u0026#34;/images/cover.jpeg\u0026#34; title=\u0026#34;Caption text.\u0026#34; \u0026gt;}} Custom Domain Serving your site from a different domain than username.github.io is relatively straightforward6. You can buy a domain at any registrar, e.g. Google Domains.\nThen go to \u0026lsquo;Pages\u0026rsquo; section within the \u0026lsquo;Settings\u0026rsquo; of your GitHub Pages repository. Here you can add your domain. Both, apex domain (example.com) or a subdomain (www.example.com or blog.example.com) are possible.\nThen link your domain to GitHub Pages7. On Google Domains, your DNS resource records should looks like the following table with example.com replaced by the domain you own and username replaced with your GitHub Pages name.\nHost Name Type TTL Data example.com A 1 hour 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 www.example.com CNAME 1 hour username.github.io For information and the latest IP addresses consult the GitHub documentation.\nIf everything is set up correctly and the record had enough time to propagate, you should see a green \u0026lsquo;✔ DNS check successful\u0026rsquo; below the \u0026lsquo;Custom Domain\u0026rsquo; section in the GitHub Pages settings.\nDon\u0026rsquo;t forget to update the baseURL in your config.yaml to your custom domain, e.g. https://www.example.com/.\nSocial Media There are various ways to optimize the display and searchability of your page on social media. You can get an idea of how your site looks when shared on social media with Metatags or LinkedIn\u0026rsquo;s post inspector.\nWhat Next Join the Hugo forum. Conclusion We\u0026rsquo;ve barely scratched the surface of what Hugo can do. Setting up a basic static site is not as easy as some commercial offers like Squarespace and Wordpress. But if you\u0026rsquo;re comfortable on the command line and using an markdown editor this setup offers you full freedom over every aspect of your site. And it\u0026rsquo;s free!\nFAQ How do I update themes? If you\u0026rsquo;ve used the git submodule add command to clone a theme repository into the themes/ folder you can preview possible updates\ngit fetch origin git status or pull updates and merge them automatically\ngit pull More info here.\nI forgot to add \u0026ndash;recurse-submodules You might not see changes appear on GitHub Pages if you forgot to add the --recurse-submodules switch when pushing the main source repository. You can rectify this by entering the submodules directory (e.g. cd public) and running git push there manually.\nMy public folder is not clean The public folder tends to accumulte folders and files form drafts and upublished content when, for example, hugo -D is run. These can be removed with Hugo\u0026rsquo;s --cleanDestinationDir option. However, this might cause issues when the public/ folder is a git submodule.\nfatal: in unpopulated submodule If you can\u0026rsquo;t seem to be able to git add files in the public/ folder anymore, you might have to remove and add the public submodule again8. This might be an issue caused by running hugo --cleanDestinationDir.\ngit submodule deinit public rm -rv public git commit -m \u0026#34;remove folder public/\u0026#34; git push # add it back, add --force if necessary git submodule add -b main https://github.com/username/username.github.io.git public git add -u git commit -m \u0026#34;add submodule \u0026#39;public\u0026#39; back\u0026#34; git push Links Static sites can be hosted in a number of locations, such as Amazon S3, Azure, CloudFront, DreamHost, Firebase, GitHub Pages, GitLab Pages, Google Cloud Storage, Heroku, Netlify, Surge.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCreating a Blog with Hugo and Github in 10 minutes\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvoiding Git Problems When Installing a Theme to Hugo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHugo: Draft, Future, and Expired Content\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCustomize a Theme\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdding a Custom Domain to your Hugo Site on Github Pages.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLinking A Custom Domain To Github Pages\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee also this link.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-03-05-how-to-create-this-blog/","summary":"\u003ch1 id=\"getting-started-with-hugo\"\u003eGetting Started with Hugo\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e is a static site generator (SSG). Simply put, an SSG prepares everything needed to display a website. In contrast, dynamically assembled websites obtain their assets only after they get accessed. An SSG thus minimizes the computational demand on the server, which allows providers, such as GitHub Pages and others\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, to host your static sites for free. More importantly, SSGs emanate a philosophy of simplicity-by-design stemming from a number of limitations. Being constraint forces content creators to focus on the content itself and minimizes the distractions of layout and design. SSGs are thus especially suited for blogs, landing pages, documentation and portfolio sites.\u003c/p\u003e","title":"How To Create This Blog"},{"content":"Data Artifacts An anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive 1. These anti-patterns can be identified in management approaches, software design, programming, and probably whenever people come together to achieve anything.\nData Science has its own set of anti-patterns that data scientist and their project managers should be aware of. “Proliferating Data Artifacts” refers to the behaviour of generating datasets which represent various stages of processing. The data scientists often have to wrangling the data into the right shape for analysis. This might include basic things like joining, filtering, mapping and aggregating data as well as any number of complex feature engineering techniques. There is a strong incentive to save the processed data after each processing step to avoid unnecessary re-processing when changes to the code affect only later steps.\nThe choice to save intermediate data artifacts can, however, become very costly when multiple versions of the same data exist. We all know this from trying to save word documents as _v1.doc, _v2.doc, etc. A colleague might email back some changes, but renames the document to _v2_John.doc destroying the whole idea of a well-defined genealogy of documents. God forbid anyone ever tries to save a document as _final.doc. Future versions with an ever-growing _final_final_... suffix are all but inevitable.\nUnlike text documents, datasets can easily take up gigabytes of space. Multiplied with the number of distinct processing steps and their variations this can grow rapidly to a level where the dreaded \u0026lsquo;out-of-space\u0026rsquo; warning hits the poor data scientist. According to Murphy\u0026rsquo;s law this will always happen just before a critical deadline. In any case, the data scientist often finds it hard to decide which dataset to delete, as he/she might be unsure which one was the latest or if it is still needed. As a consequence a disproportionate amount of time is spent on managing intermediate data artifacts and storage space.\nThe \u0026ldquo;Proliferating Data Artifacts\u0026rdquo; anti-pattern leads to the unnecessary management of intermediate data artifacts and storage space.\nPipelines to the Rescue But as with every anti-pattern — there is a solution! Pipelines. Pipelines manage intermediate datasets under-the-hood. A pipeline is basically a directed acyclic graph that describes the step-wise processing of the data.\nIt is easy to see that any data artifact could be recreated by applying all the processing steps (arrows) that lead to it to the preceding data. All that is needed is the input data and a well-defined pipeline.\nThe pipeline approach has several advantages:\nThe user does not need to track, save, or pass on (large) data artifacts. Unnecessary re-processing of unchanged/unaffected data artifacts is avoided. A pipeline is defined in code and can be versioned. Data artifacts from any pipeline version can easily and reproducibly be re-created. There are many more advantages that a good pipeline implementation can offer, such as parallel processing of independent parts of the pipeline, being self-contained, restart after failure, and efficient propagation of data changes.\nSo there\u0026rsquo;s really no excuse not to use existing pipeline frameworks (or implement them yourself) and delete those proliferating data artifacts! 2\nWikipedia: Anti-Pattern\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe original version of this text was published 2018-05-09.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2021-11-16-data-science-anti-patterns/","summary":"\u003ch2 id=\"data-artifacts\"\u003eData Artifacts\u003c/h2\u003e\n\u003cp\u003eAn anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. These anti-patterns can be identified in management approaches, software design, programming, and probably whenever people come together to achieve anything.\u003c/p\u003e\n\u003cp\u003eData Science has its own set of anti-patterns that data scientist and their project managers should be aware of. “Proliferating Data Artifacts” refers to the behaviour of generating datasets which represent various stages of processing. The data scientists often have to wrangling the data into the right shape for analysis. This might include basic things like joining, filtering, mapping and aggregating data as well as any number of complex feature engineering techniques. There is a strong incentive to save the processed data after each processing step to avoid unnecessary re-processing when changes to the code affect only later steps.\u003c/p\u003e","title":"Data Science Anti-Patterns: Proliferating Data Artifacts"},{"content":"1. Serve your model quickly. The ultimate measure of success is that the insights generated by your model are useful to someone. The sooner that someone looks at the model output the sooner he/she can provide valuable feedback. I\u0026rsquo;d not be surprised if currently the vast majority of advanced analytics projects are shelved or abandoned, because the predictions cannot be integrated into business processes in a meaningful way. Or something is predicted that was already known by other means. A quickly-served model will also help you answer all your questions around performance, visualization, and production environment. As a result you\u0026rsquo;re less likely to optimize prematurely, over-deliver, and you can ensure that the model can continue to exist past the end of the project.\n2. Data Science projects are inherently iterative. Data quality can usually not be determined before modelling and model experimentation might affect data acquisition and pre-processing. Going through these iterations is often more efficient than trying to ascertain truths about data quality and model choice in advance. Many clients are worried about the quality and quantity of their data and might want to delay the start of a data science project until they\u0026rsquo;re sure that success is guaranteed. However, the most efficient way to determine data quality is by modelling.\nData science projects are people projects.\n3. Data Science projects are similar to software projects. They benefit from team spirit, improved communication, and a culture that encourages experimentation and learning. Clients are often concerned that they\u0026rsquo;re not getting the ‘best’ model and the ‘most accurate’ prediction. But the fact that the speed and quality of delivery could be easily improved with a few simple measures is often forgotten. In the end, data science projects are people projects. Things like a distraction-free work environment, easy access to an adequate development environment, ergonomic workplace and hardware, and easy access to data, have a bigger impact on success than a particular model choice.\nBonus Rule: Chose technology and tooling to support all of the above.\nThree quick rules for successful Data Science projects. Interested readers might want to learn about a structured Data Science approach like CRISP-DM 1 next. 2\nCRoss Industry Standard Process for Data Mining (CRISP-DM)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe original version of this text was published 2018-02-08.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2021-08-03-three-rules-of-success/","summary":"\u003ch2 id=\"1-serve-your-model-quickly\"\u003e1. Serve your model quickly.\u003c/h2\u003e\n\u003cp\u003eThe ultimate measure of success is that the insights generated by your model are useful to someone. The sooner that someone looks at the model output the sooner he/she can provide valuable feedback. I\u0026rsquo;d not be surprised if currently the vast majority of advanced analytics projects are shelved or abandoned, because the predictions cannot be integrated into business processes in a meaningful way. Or something is predicted that was already known by other means. A quickly-served model will also help you answer all your questions around performance, visualization, and production environment. As a result you\u0026rsquo;re less likely to optimize prematurely, over-deliver, and you can ensure that the model can continue to exist past the end of the project.\u003c/p\u003e","title":"Three Rules of Success for Data Science Projects"}]