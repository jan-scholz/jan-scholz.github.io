[{"content":"The lacklustre reception of the recent GPT-5 release1 (Aug 2025) was a reminder of the slowing progress we can expect from future LLMs. The current LLM design has fundamental limits when it comes to achieving artificial general intelligence (AGI).\nBesides not being able to learn easily after their very long and expensive training runs have completed, current LLMs seem unable to count2 or do even simple arithmetic3. They fail at algorithmic reasoning tasks4 and give wrong responses with confidence (i.e. hallucinate). And they just can‚Äôt seem to follow the rules of chess5!\nTherefore, most chatbot providers have now pivoted to agent-like systems that enrich the LLM\u0026rsquo;s context with past conversations, chain-of-thought, and tools. These tools are specifically designed to patch weaknesses of LLMs, such as math and code execution.\nThis sounds an awful lot like what I call the \u0026ldquo;Frankenstein stage of technology development\u0026rdquo;. There is little left to squeeze out of the core idea and the only progress consists of bolting on more stuff. Kind of like when mobile phones just started to get more cameras after the initial rapid improvements in terms of screen size, processing power and miniaturization had reached their limits6.\nAgents. A Blast from the Past. The idea of an \u0026ldquo;agent\u0026rdquo; was developed over 50 years ago to describe learning behaviour that involved repeated interaction with the environment and feedback or rewards. Both, psychology and early AI (or cybernetics) research adopted the concept and refined it7.\nRichard S. Sutton formalized the concept of an autonomous agent in his 1983 PhD dissertation8: An agent interacts with an environment in discrete time steps $t$. The agent\u0026rsquo;s entire knowledge and perception of the environment (at time $t$) is captured in state $S_t$. Based on this state, the agent selects an action $a_t$. The agent then receives a state update $S_{t+1}$ and a reward $r_{t+1}$, which is key for reinforcement learning (RL).\nThe basic agent-environment interaction loop.\nModern LLM-based chatbots (or so called \u0026ldquo;agentic\u0026rdquo; AI) draw on some but not all of the characteristics of this classic RL agent. They interact with the environment (i.e. the user) in discrete steps and their actions consist of generating tokens. Chatbots also update their state, e.g. by saving the user\u0026rsquo;s response to their conversational history. Additionally, they \u0026ldquo;act\u0026rdquo; by using tools and their \u0026ldquo;environment\u0026rdquo; can be their own output during chain-of-thought.\nHowever, chatbots deviate from Sutton\u0026rsquo;s classic RL agent definition due to the lack of continuous online learning. Chatbots are mostly \u0026ldquo;fixed\u0026rdquo; at inference time. Reinforcement learning from human feedback (RLHF) is used during LLM pre-training, but not during the chat session.\nThe thumbs up/down that users can provide and other session metrics are probably anonymized and aggregated before being used for offline model fine-tuning. The closest chatbots come to learning within session is the way they change their behaviour based on the chat history and some user preferences (e.g. system prompt). But anyone who has tried to correct a chatbot knows the limits of the teaching-via-context approach. Without weight adjustments chatbots tend to eventually revert back to their original behaviour.\nConclusion LLM progress has slowed even while the effort to improve the paradigm has increased tremendously. Currently billions of dollars are being invested in squeezing out marginal gains that don‚Äôt fundamentally make LLMs more intelligent9. This fact alone should give anyone pause who was hoping that LLMs could become AGI.\nOf course, it\u0026rsquo;s possible that there will be another game-changer like the transformer architecture10, but this is increasingly unlikely given that thousands of publications each year haven\u0026rsquo;t delivered any major breakthrough.\nThe autoregressive transformer architecture11 does not seem to be suitable for true intelligence, which is the ability to \u0026ldquo;achieve goals in a wide range of (novel) environments\u0026rdquo;12. For now, LLMs mostly retrieve and interpolate between the language examples they were trained on. So by definition, they cannot generalize and give responses that are outside or in conflict with the training data.\nWhile the LLM-chatbot loop sounds similar to the classic RL agent‚Äìenvironment interaction paradigm, it still differs fundamentally. Chatbots don\u0026rsquo;t learn online and even if they did it\u0026rsquo;s not clear if the classic RL agent approach would be feasible given the large number of states such a system could be in. I\u0026rsquo;m skeptical that LLMs themselves will be the key to AGI, despite the advances of LLM-based agents over \u0026ldquo;naked\u0026rdquo; LLMs.\nLinks ChatGPT users hate GPT-5‚Äôs \u0026ldquo;overworked secretary\u0026rdquo; energy, miss their GPT-4o buddy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026ldquo;Strawberry\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIs OpenAI\u0026rsquo;s o1 a good calculator?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLimitations of Language Models in Arithmetic and Symbolic Induction\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGPT-5 and GPT-5 Thinking as Other LLMs in Chess: Illegal Move After 4th Turn\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnother example of the Frankenstein stage of development was the auto-ML craze, when the data science industry became fixated on training every available model under the sun with every possible feature combination (2016). This brute-force approach might have worked well to win Kaggle competitions, but it also signalled that traditional machine-learning techniques had run out of fresh ideas. Lacking innovation, the industry leaned on doing more rather than doing better. Ironically, the companies building these auto-ML systems still profited, since the trend drove demand for more compute, storage, and infrastructure. (ü§î Hmmm. These are some interesting parallels to the current chatbot industry.)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe conceptual foundations of the agent-environment model were emerging well before modern reinforcement learning. In cybernetics (Norbert Wiener, 1948) and optimal control theory (Bellman, Kalman, Pontryagin), researchers explored how systems could adapt their behaviour to achieve goals in dynamic environments through feedback loops. In animal learning theory, psychologists and neuropsychologists framed behaviour as a process of stimulus-response interactions shaped by reinforcement (Thorndike, Skinner, Hebb). They theorized that intelligent behaviour arises from continuous interaction between an agent and its environment, where actions influence future observations and rewards.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1983). Temporal credit assignment in reinforcement learning. Doctoral dissertation, University of Massachusetts Amherst.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example: Quantization, KV-cache optimization, Mixture-of-Experts, speculative decoding, distillation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., \u0026amp; Polosukhin, I. (2017). Attention is all you need. arXiv.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTo be more precise: autoregressive deep learning multi-head self-attention transformer architecture with tokenized input/output.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFran√ßois Chollet\u0026rsquo;s \u0026ldquo;On the Measure of Intelligence\u0026rdquo; (2019): \u0026ldquo;Intelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments ‚Äî including ones it has never encountered before.\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2025-09-11-llms-are-done/","summary":"\u003cp\u003eThe lacklustre reception of the recent GPT-5 release\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e (Aug 2025) was a reminder of the slowing progress we can expect from future LLMs. The current LLM design has fundamental limits when it comes to achieving artificial general intelligence (AGI).\u003c/p\u003e\n\u003cp\u003eBesides not being able to learn easily after their very long and expensive training runs have completed, current LLMs seem unable to count\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e or do even simple arithmetic\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e. They fail at algorithmic reasoning tasks\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e and give wrong responses with confidence (i.e. hallucinate). And they just can‚Äôt seem to follow the rules of chess\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e!\u003c/p\u003e","title":"LLMs Are Done. What Are Agents?"},{"content":" How do you build a production-ready app in only two days? Do it four times.\nYes, that\u0026rsquo;s technically eight days! But hear me out. In my previous post I argued that Building Software Is No Longer a Nightmare, because AI coding assistants can take care of all the annoying peripheral tasks like packaging, testing, documentation, deployment, etc.\nHere I want to propose that AI-assisted coding makes it easier than ever to return to the ultimate form of software engineering: \u0026ldquo;Throwaway Prototyping\u0026rdquo;. This concept was pioneered by Fred Brooks in his classic book \u0026ldquo;The Mythical Man-Month\u0026rdquo;1, where he coined the phrase, \u0026ldquo;Plan to throw one away; you will, anyhow.\u0026rdquo; Brooks refers specifically to the first iteration of a system \u0026ndash; the prototype. I will expand this definition to the practice of discarding code and architecture multiple times till the problem and its solution are sufficiently understood.\nDiscarding code, refactoring, and revising architecture is often taken to be implicitly part of Agile methodology and Extreme Programming. However, I\u0026rsquo;ve seen it rarely practiced at enterprise due to the assumed high cost of throwing away working code, and the perceived lack-of-time to rebuild from scratch. Fortunately, all of these objections have become moot thanks to AI if one is willing to bear the token expense, that is.\nAI-assisted Throwaway Prototyping is one of the best ways to allow a developer to achieve what used to require a small team of dedicated experts. This requires the developer to actively attempt to go beyond their limitations, i.e. leave their comfort zone.\nUsing AI to develop within your comfort zone will merely accelerate your code output, developing into unfamiliar territory will accelerate your product delivery.\nI\u0026rsquo;ve found that a typical project using the Throwaway Prototyping approach tends to have four distinct phases each lasting 1-3 days. Each phase advances the problem understanding and solution approach. This is crucial: Throwaway Prototyping is meant to advance your understanding of the problem, unlike Vibe Coding which absolves the \u0026ldquo;developer\u0026rdquo; from this duty entirely.\nI\u0026rsquo;ll describe the four phases of Throwaway Prototyping by using an example scenario. Let\u0026rsquo;s say you are an experienced Python developer with little JS experience. You are tasked with building a real-time audio communication app.\nPhase 1: Comfort Zone + Personal Bias You start in your comfort zone using the tools you are familiar with. As a Python developer you are familiar with FastAPI backend development. You focus on building the backend of the application, taking care of implementing the right routes. The way you understand the problem, you try to solve it with websockets and real-time streaming. Your bias tells you that you should solve this problem using object-oriented programming.\nAfter two days, you realize that your beautifully designed architecture and clean code doesn\u0026rsquo;t really work. The audio processing is unreliable, possibly due to lost packets. With a heavy heart you decide to throw away the first prototype.\nPhase 2: Comfort Zone + Overcoming Personal Bias The first prototype taught you that the real-time streaming approach is overly complex and not really necessary to make the application work.\nYou start anew, careful to overcome your biases. You focus on the core issue and re-design the backend communication from scratch. A much simpler approach of synchronous data exchange using the POST method is more robust while not substantially increasing latencies.\nNow your focus shifts to the frontend. With little JavaScript experience, you realize you are in over your head. While the backend is rock-solid and responds reliably when tested in isolation, the frontend is brittle and sluggish. You throw away the entire frontend code base.\nPhase 3: Stretch Zone + AI Bias Your second prototype taught you that the problem can be solved using synchronous communication and that you lack FE experience.\nYou leverage AI to revise the frontend architecture and rebuild it from scratch. Your solid backend architecture together with the well-defined routes serve as design specifications. These can be can be used to prompt the AI to select the right frameworks and implementation approach for the frontend. However, the AI has its own biases and the code grows organically as you try to nudge it toward your envisioned user experience.\nYou‚Äôre impressed! The AI got much closer to building a responsive, feature-rich application than you ever could have. But the frontend codebase has evolved into a mess. The AI scattershot changes across the entire code base whenever you asked it to address issues or add new features. You are unable to keep up with the AI and start to worry about the rapidly accumulating technical debt.\nYou don\u0026rsquo;t know who you should be more disgusted with, the AI or yourself. How could you have let things devolve so quickly in the pursuit of rapid development? You throw away the third prototype.\nPhase 4: Stretch Zone + Overcoming AI Bias You now have a clearer picture of what the frontend architecture needs to accomplish. Working together with the AI coding assistant on the third prototype taught you a lot about frontend engineering. Your prompts are now more informed and you are able to direct the AI on choices like signals vs observer patterns, template literals vs web components, etc.\nA mature backend is now joined with a clean and functional frontend. AI assistance allowed you to repeatedly throw away code and start from scratch. This way you were able to incorporate new learnings at each iteration free from the constraints of pre-existing code.\nConclusion There is no doubt that AI-assisted coding is already transforming the industry. A tsunami of AI-generated slop will be one outcome of this development. I\u0026rsquo;m hopeful that we can also use AI to grow as engineers while building better products. My version of AI-assisted Throwaway Prototyping illustrates how this human-AI collaboration could look like:\nAI-Assisted Throwaway Prototyping 1. Comfort zone +\nBias 2. Comfort Zone +\nOvercome Bias 3. Stretch Zone +\nAI Bias 4. Stretch Zone +\nOvercome AI Bias Use what you know Solve problem your way Use what you know Solve problem based on what you learned previously Use AI to extend out of comfort zone Accept AI bias Use AI to extend out of comfort zone Direct AI to overcome its (default) bias \u0026copy; 2024 talesofindustry.org Internet Archive - The Mythical Man-Month by Frederick P. Brooks Jr.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2025-07-03-how-to-build-a-production-ready-application-in-two-days/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eHow do you build a production-ready app in only two days?\u003c/em\u003e \u003c/br\u003e\n\u003cem\u003eDo it four times.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eYes, that\u0026rsquo;s technically eight days! But hear me out. In my previous post I argued that \u003ca href=\"https://www.talesofindustry.org/post/2025-06-16-building-software-is-no-longer-a-nightmare/\"\u003eBuilding Software Is No Longer a Nightmare\u003c/a\u003e, because AI coding assistants can take care of all the annoying peripheral tasks like packaging, testing, documentation, deployment, etc.\u003c/p\u003e\n\u003cp\u003eHere I want to propose that AI-assisted coding makes it easier than ever to return to the ultimate form of software engineering: \u0026ldquo;Throwaway Prototyping\u0026rdquo;. This concept was pioneered by Fred Brooks in his classic book \u0026ldquo;The Mythical Man-Month\u0026rdquo;\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, where he coined the phrase, \u0026ldquo;Plan to throw one away; you will, anyhow.\u0026rdquo; Brooks refers specifically to the first iteration of a system \u0026ndash; the prototype. I will expand this definition to the practice of discarding code and architecture \u003cem\u003emultiple\u003c/em\u003e times till the problem and its solution are sufficiently understood.\u003c/p\u003e","title":"How to Build a Production-Ready Application in Two Days"},{"content":"Will Large Language Models make us better coders or worse? While this questions will take time to settle, one thing it clear: LLMs have finally broken down the barriers to creating and publishing open source. How many projects took off quickly with the implementation of the core logic only to run into a wall once it came to \u0026ldquo;all the other stuff\u0026rdquo;, such as:\nVersion control \u0026amp; hosting Packaging Publication Testing Documentation UI/UX Your experience might differ, maybe you\u0026rsquo;re a git-whiz who is cranking out command-line-foo like there\u0026rsquo;s no tomorrow. Or maybe you honed the perfect setup to scaffold and automate tasks like packaging or testing over the years. Lucky you! But the majority of developers are grinding every day through an ungodly mess of confusing git commands, dependency issues, and contradictory testing practices. And I won\u0026rsquo;t even go into the hell that is frontend development for the sake of our collective sanity 1.\nMost programmers probably spend more time on peripheral issues than they are prepared to admit. It\u0026rsquo;s not too unusual to burn an entire afternoon on resolving version conflicts, or desperately trying out random configuration options. Of course, none of this ever felt like it had anything to do with solving the actual task at hand. No new feature, no performance improvement came out of these excursions, just unadulterated frustration and a sense of being an imposter. Was anyone really living up to their CS degrees when they were blindly mashing keys like a monkey on a typewriter?\nFortunately, if nothing else, LLMs have undoubtedly broken down the penumbra of semi-professional software development2. What used to take days of trudging through Reddit posts, GitHub discussions, and trying to read pay-walled Medium posts is now just a prompt away. In the olden days, building applications and publishing them online required solving a whole range of unrelated problems from which our multi-headed overlords have finally freed us.\nWhy was it so hard to begin with? There are probably three major sources of frustration for any hobbyist or semi-professional developer.\nToo many languages (and frameworks) The sheer amount of programming and scripting languages would turn any normal person\u0026rsquo;s brain to mush. Ever heard of an interpreter speaking more than 10 languages? But that is what programmers are faced with. Think back to all the programming, scripting, configuration, automation, markup, style and query languages you\u0026rsquo;ve come across3!\nHobbyist developers can\u0026rsquo;t afford to develop all the necessary code from scratch and are thus reliant on pre-existing libraries and frameworks. Modifying, configuring, or integrating with 3rd party code thus often requires becoming familiar with the developer\u0026rsquo;s favorite language and framework.\nPoor documentation Most of us have probably come across cases where we just couldn\u0026rsquo;t get something to work until we found that one crucial hint in some random tutorial or GitHub issue discussion. It\u0026rsquo;s rare to come across documentation that is complete and updated regularly. When even Google struggles, what can we really expect from open source projects?\nRapidly evolving ecosystem Keeping up with new languages, frameworks, and best practices can be overwhelming. Developers constantly have to decide whether to hone the skills they already have or learn a new thing. It feels like a gamble that one should not be required to make. There is a reason why open source developers burn out quickly and abandon projects. The ever changing ecosystem with its fads and breaking changes makes it hard to hold on to what works and is well-understood.\nOpen source is more fun with LLMs LLMs allow anyone to focus on implementing their core idea again rather than suffering through learning a new language for a one-off project, figuring out packaging \u0026amp; distribution, setting up testing and so on.\nHow does LLM-assisted coding deliver us from the nightmare that is modern software development?\nLLMs make everything accessible via human language. LLMs can instantly explain any piece of code to you or translate it into a programming language you know. LLMs can implement your human-language specifications into any programming language. Using human language to read and write code greatly reduces \u0026ldquo;language barriers\u0026rdquo;.\nLLMs filter out the noise. LLMs gather answers across many sources, including documentation, blog posts, and forums and distill it down to your specific circumstances with summaries and example code. With the right prompt they give very straight answers4 making you wonder why you ever read through pages of Stackoverflow answers arguing about the relevancy or duplication status of a question.\nLLMs don\u0026rsquo;t shill. LLMs don\u0026rsquo;t pressure you into adopting a new framework. They mostly base their recommendations on your requirements or what fits with your existing code base.\nLLMs help iterate. Have you ever felt stuck? Sometimes nothing seems to work. Now, if something fails, let the LLM respond to the error message, diagnose the issue and recommend possible solutions. Still can\u0026rsquo;t get it to work? Pivot! Try a different framework/library. Have the LLM translate the existing code to fit with the new dependency. Or let the LLM build custom code instead of hunting for a library that works with your code.\nCaveats Of course, not all is rosy in LLM land. LLM responses get better the more context is provided. Like humans, LLMs can misinterpret your question, over-complicate solutions, forget, and go off on tangents. Thus you should use git liberally. Commit changes before every prompt and don\u0026rsquo;t be afraid to revert to previous versions that worked and try again with a better prompt based on what you learned. I found that the following rules achieve overall better results.\nProvide motivation to your question. What are you trying to achieve overall? \u0026quot;I am building a chat app with a FastAPI backend and a React frontend. What is the best way to ...\u0026quot; Ask broader questions first. Ask for options and review the pros \u0026amp; cons.\u0026quot;\u0026quot; \u0026quot;What are the 2 most popular Python build backends.\u0026quot; Narrow down the tech stack and emphasize what options you picked.\u0026quot;\u0026quot; \u0026quot;I want to use hatch. How can I create a dedicated testing environment.\u0026quot; Don\u0026rsquo;t be afraid to backtrack. LLMs can take you into the wrong direction. \u0026quot;Your suggestion to ... is too complicated/does not work. I reverted all changes.\u0026quot; Break up more complex tasks. Ask to make a plan first, which is then in the LLM\u0026rsquo;s context window to guide further responses. \u0026quot;[Complex question]. Make a plan first, don't implement anything yet.\u0026quot; Let the LLM do the busy work. \u0026quot;Create a unit test for [code]. Create a docstring for [code].\u0026quot; These are just some general rules. Obviously, an editor with an integrated agent that has full access to you source code will behave differently than editor-to-LLM copy-and-pasting. (I might discuss specific tools in a follow-up article.)\nConclusion It has become a whole lot easier to share self-developed software. Tasks (such as packaging, publication, testing and documentation) which used to distract from the core development have become infinitely more accessible with LLMs.\nI\u0026rsquo;ve added some query-answer pairs below for illustration. (Some content has been removed for brevity.)\n‚ûî ‚úÖ 1. Project Structure Your directory should look something like this:\nyour_package/ ‚îú‚îÄ‚îÄ your_package/ # Main source code ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îî‚îÄ‚îÄ ...other modules... ‚îú‚îÄ‚îÄ tests/ # (optional but recommended) ‚îÇ ‚îî‚îÄ‚îÄ test_something.py ‚îú‚îÄ‚îÄ pyproject.toml # Configuration (modern standard) ‚îú‚îÄ‚îÄ README.md # Project description ‚îú‚îÄ‚îÄ LICENSE # Choose a license ‚îî‚îÄ‚îÄ .gitignore # (optional, for development) ‚úÖ 2. pyproject.toml Configuration This file replaces setup.py in modern packaging. Here‚Äôs a minimal example using setuptools:\n[build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; [project] name = \u0026#34;your-package-name\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;A short description\u0026#34; readme = \u0026#34;README.md\u0026#34; license = {text = \u0026#34;MIT\u0026#34;} authors = [{name = \u0026#34;Your Name\u0026#34;, email = \u0026#34;you@example.com\u0026#34;}] dependencies = [ \u0026#34;requests\u0026gt;=2.0\u0026#34;, # Example dependency ] ‚úÖ 3. Write a README and include a License\n[..]\n‚úÖ 4. Create a Source Distribution and Wheel\npython -m pip install build python -m build This generates dist/your_package_name-version.tar.gz and .whl files.\n‚úÖ 5. Register \u0026amp; Upload to PyPI\n[..]\n‚úÖ 6. Test Installation\n[..]\n‚ûî You use the @pytest.mark.parametrize decorator.\nBasic example:\nimport pytest @pytest.mark.parametrize( \u0026#34;input, expected\u0026#34;, [ (2, 4), (3, 9), (4, 16), ] ) def test_square(input, expected): assert input * input == expected Here‚Äôs what happens:\nThe test test_square will run 3 times. Each time, input and expected take values from the list of tuples. The test checks that squaring the input gives the expected result. [..]\n‚ûî Prepare your Python package Make sure your Python package is ready with: setup.py or pyproject.toml Versioning handled properly (e.g., via setuptools_scm or manually updating version) Your package builds correctly (python setup.py sdist bdist_wheel or python -m build) Create a PyPI API token and add it to GitHub Secrets [..]\nCreate the GitHub Actions workflow In your repo, create a .github/workflows/publish.yml file with the following example workflow: name: Publish Python Package on: push: tags: - \u0026#39;v*.*.*\u0026#39; # Trigger on version tags like v1.2.3 jobs: publish: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: \u0026#39;3.x\u0026#39; - name: Install build tools run: | python -m pip install --upgrade pip pip install build twine - name: Build package run: python -m build - name: Publish to PyPI env: TWINE_USERNAME: __token__ TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }} run: | python -m twine upload dist/* Tag your release and push Create a new tag for the version you want to publish (matching vX.Y.Z pattern): git tag v1.0.0 git push origin v1.0.0 Once you push the tag, GitHub Actions will trigger and publish your package to PyPI automatically.\n[..]\nVanilla, React, Angular, Vue, Svelte, Preact, Lit, or Solid? JS or TS? JS and CSS inside HTML? Jinja templates? State management? Light and dark mode? Did I say I wanted a button!? I don\u0026rsquo;t want that button anymore. I\u0026rsquo;ll just sit here and stare at the wall for a bit.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;ve purposefully left out more \u0026ldquo;professional\u0026rdquo; topics like logging, security, and infrastructure. I\u0026rsquo;m also skipping over type hints and refactoring.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMost used programming languages among developers worldwide as of 2024\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf at all, LLMs can appear a little too confident.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2025-06-16-building-software-is-no-longer-a-nightmare/","summary":"\u003cp\u003eWill Large Language Models make us better coders or worse? While this questions will take time to settle, one thing it clear: LLMs have finally broken down the barriers to creating and publishing open source. How many projects took off quickly with the implementation of the core logic only to run into a wall once it came to \u0026ldquo;all the other stuff\u0026rdquo;, such as:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eVersion control \u0026amp; hosting\u003c/li\u003e\n\u003cli\u003ePackaging\u003c/li\u003e\n\u003cli\u003ePublication\u003c/li\u003e\n\u003cli\u003eTesting\u003c/li\u003e\n\u003cli\u003eDocumentation\u003c/li\u003e\n\u003cli\u003eUI/UX\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYour experience might differ, maybe you\u0026rsquo;re a git-whiz who is cranking out command-line-foo like there\u0026rsquo;s no tomorrow. Or maybe you honed the perfect setup to scaffold and automate tasks like packaging or testing over the years. Lucky you! But the majority of developers are grinding every day through an ungodly mess of confusing git commands, dependency issues, and contradictory testing practices. And I won\u0026rsquo;t even go into the hell that is frontend development for the sake of our collective sanity \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"Building Software Is No Longer a Nightmare"},{"content":"It was an absolute pleasure to be invited to present at the Machine Learning Toronto Meetup.\nHuge shout-out to Myles of NLP from Scratch-fame and the other organizers for hosting such an amazing meetup. Abhi delivered a breathtaking deep dive into optimizing information retrieval pipelines down to the CPU level.\nThat was a tough act to follow with my \u0026ldquo;Journey to develop user-friendly applications that provide insights at enterprise scale\u0026rdquo;. But I was glad to see it sparked some interesting conversations around how to make time for teams to learn and how to practice domain-driven design. Here are my slides.\n","permalink":"https://www.talesofindustry.org/post/2024-09-30-mlto-talk-ai-product-development/","summary":"\u003cp\u003eIt was an absolute pleasure to be invited to present at the Machine Learning Toronto Meetup.\u003c/p\u003e\n\u003cp\u003eHuge shout-out to \u003ca href=\"https://www.linkedin.com/in/mylesharrison/\"\u003eMyles\u003c/a\u003e of \u003ca href=\"https://www.nlpfromscratch.com\"\u003eNLP from Scratch\u003c/a\u003e-fame and the other organizers for hosting such an amazing \u003ca href=\"https://www.meetup.com/machine-learning-to-meetup/events/302696444/\"\u003emeetup\u003c/a\u003e. \u003ca href=\"https://www.linkedin.com/in/abhimanyu-anand/\"\u003eAbhi\u003c/a\u003e delivered a breathtaking deep dive into optimizing information retrieval pipelines down to the CPU level.\u003c/p\u003e\n\u003cp\u003eThat was a tough act to follow with my \u0026ldquo;Journey to develop user-friendly applications that provide insights at enterprise scale\u0026rdquo;. But I was glad to see it sparked some interesting conversations around how to make time for teams to learn and how to practice domain-driven design. Here are my \u003ca href=\"/docs/Tales%20of%20Industry%20-%20AI%20Product%20Development.pdf\"\u003eslides\u003c/a\u003e.\u003c/p\u003e","title":"MLTO Talk: What I learned about AI Product Development"},{"content":"Introduction The tech industry is far from being as diverse as it should be. Women make up around 23-25% of the US \u0026ldquo;tech workforce\u0026rdquo; in major tech companies1. These numbers are well below the total participation rate of women in these companies (29-45%). The situation is even worse for the field of data science, where \u0026ldquo;of all the various tech fields, data science currently ranks the lowest in diversity\u0026rdquo;2. It\u0026rsquo;s not difficult to imagine that things are not much better for data science teams in large enterprises such as banks, telcos and retailers. With all the caveats associated with online surveys, the data suggests that only about 20% of data scientists across industries are women3.\nIf organizations reduce diversity to a matter of meeting quotas, they fail to understand that diverse teams are essential to solving complex challenges like AI.\nThese numbers paint a dire picture of the AI industry without even considering other kinds of diversity. In addition to the important question of equity, diverse teams are also more innovative and adapt more easily to a rapidly changing environment4. A quota-focused approach that disregards the inherent value of teams with varied backgrounds fails individuals and organizations alike. In the following, I will look at diversity through three distinct lenses, aiming to unveil a more nuanced perspective that can help organizations find a better approach to diversity.\nNote, I\u0026rsquo;m not arguing that one kind of diversity is more important than another. Organizations should strive to address all of the following and more.\n1. Gender and Cultural Diversity Gender and culture dominate the diversity discussion. Plans to increase diversity are often measured by how they address the gender ratio and the cultural (including ethnic, racial, and religious) markup of the employee base. Overall, the tech industry has made huge efforts to attract more women, LGBTQ+ members, and people of colour by creating better training opportunities, adjusting their talent search, and making the workplace more inclusive5.\nVarious theories, such as the \u0026ldquo;leaky pipeline theory\u0026rdquo; and the \u0026ldquo;hostile workplace theory\u0026rdquo; have been brought forward to explain the persistent gender gap6. And structural barriers keep certain minorities from participating according to their labor participation rate. Meta, Google, Apple and others are backing affirmative action7 to increase diversity in higher education and thus their talent pools.\nGender and cultural diversity are the most visible markers of diversity as they can be communicated as part of one\u0026rsquo;s identity. A lack of diversity along these dimensions is thus more apparent, making gender and ethnicity targets for diversity quotas. However, organizations that focus on a few visible markers of diversity might get stuck at a very shallow understanding of the \u0026ldquo;why\u0026rdquo;.\nThis is ostensibly the case when increasing diversity is seen as a cost to the business. Too many organizations rely on employees to \u0026ldquo;volunteer\u0026rdquo; and organize events with the occasional external diversity representative being invited as a guest speaker. Unfortunately, this can mislead members of these organizations into thinking that diversity is meaningfully addressed, because these events happen.\nIt becomes quickly apparent how interested an organization is in promoting diversity, when actual change and investment are required. Is money spent to find and recruit women, LGBTQ+ members, and people of colour? Or does the organization put the burden on candidates to present themselves that way on LinkedIn? Are external communities and training organizations supported to help increase the talent pool for the whole industry? How about accessibility, accommodations, and promotions?\nAll of the above requires meaningful financial and organizational commitment. Deepening the understanding of what diversity really means can help avoid seeing it as a prescribed target that has to be achieved in the most cost-effective manner.\n2. Cognitive \u0026amp; Behavioural Diversity A diversity approach that is monomaniacally focused on achieving quotas misses additional dimensions of diversity. While gender and cultural diversity have been shown to be beneficial for organizations, this has been attributed to the fact that teams with people who think differently and do things differently are more creative at problem solving. A team or organization with low cognitive diversity can suffer from \u0026ldquo;groupthink\u0026rdquo; which leads to poor decision making and the failure to address risks. Put differently:\nHomogeneous teams produce homogeneous outcomes8.\nTaking cognitive diversity into account presents another opportunity for organizations to push themselves to create an overall better and more accommodating workplace. For example, individuals who tend to be more introverted might get overlooked in organizations which have a culture of dominance. These organizations deprive themselves of the value that introverted individuals can contribute, while at the same time failing to provide an equitable workplace.\nCognitive and behavioural diversities refer to the different ways that individuals perceive the world, learn, make decisions, communicate, and act. Some of these characteristics have been popularized by the various (pseudoscientific) personality tests that large organizations and consultancies are so enamoured with. Independently of what the benefits of particular cognitive and behavioural traits are, it should be apparent that there is often no \u0026ldquo;best way\u0026rdquo; of doing things when it comes to solving complex, novel problems such as AI. Thus, teams that can explore multiple approaches have a better chance of discovering solutions.\nCognitive diversity is important at all levels of hierarchy. It helps improve decision making, increases adaptability in a fast changing environment, enhances strategic anticipation, and promotes innovation4; all of which are vital for AI adoption.\n3. Diversity of Experience The different kinds of diversity described above are obviously not mutually exclusive. Gender and cultural diversity are probably correlated with cognitive diversity due to differences in upbringing, schooling, and social expectations. It\u0026rsquo;s thus not wrong in itself for an organization to address more visible markers of diversity.\nHowever, focusing on a few select markers of diversity can distract from the more expansive definition of what I will call \u0026ldquo;diversity of experience\u0026rdquo; for short. Ultimately, an organization (as an entity with a purpose) should have a self-interest in being accommodating to individuals with a wide range of experiences to avoid getting trapped in the thinking and decision making of a particular group.\nThe fact that we are still far away from achieving diversity goals \u0026ndash; even in the AI industry that would benefit the most \u0026ndash; suggests that Goodhart\u0026rsquo;s Law9 is still alive and strong. Quotas and metrics can help an organization steer in the right direction, but they come with the risk of simplifying things and overlooking the depth and complexity of the challenge. Metrics will be \u0026ldquo;gamed\u0026rdquo; and perceived as another box to tick with the least amount of effort possible.\nA wholesome appreciation of the value that \u0026ldquo;diversity of experience\u0026rdquo; brings to a team could help all members of an organization unite around a common goal of increasing diversity.\nConclusion Unfortunately, the tech sector \u0026ndash; and the AI industry in particular \u0026ndash; are still less diverse than the general labour force. This is surprising, because diverse teams are essential to solving complex challenges like AI.\nEquity is often presented in a language of social discourse that enterprises find challenging to integrate into their hiring and workplace processes. They live in a world of key performance metrics, quotas, budgets, and cost optimization.\nI tried to outline a complementary framework that could serve as a counter-narrative to the prevailing surface-level understanding of diversity. What if diversity is desirable in and of itself for an organization, because it has value? It should thus be in every organization\u0026rsquo;s self-interest to provide a workplace that is equitable and attracts diversity.\nWomen\u0026rsquo;s Representation in Big Tech\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCrunching the Numbers on Diversity in Data Science: Events \u0026amp; Resources to Foster Inclusion\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow are the üíÉLadies and the üé©Gents doing?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeams Solve Problems Faster When They‚Äôre More Cognitively Diverse\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDiversity in Data Science: A Systemic Inequality\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGender Diversity in the Tech Industry ‚Äî What does the literature say?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBig Tech lends its support in Harvard affirmative action case\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow to enable creativity and innovation: Work with diverse teams in an inclusive culture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoodhart\u0026rsquo;s law is an adage often stated as, \u0026ldquo;When a measure becomes a target, it ceases to be a good measure\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2024-02-10-three-kinds-of-diversity/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThe tech industry is far from being as diverse as it should be. Women make up around 23-25% of the US \u0026ldquo;tech workforce\u0026rdquo; in major tech companies\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. These numbers are well below the total participation rate of women in these companies (29-45%). The situation is even worse for the field of data science, where \u0026ldquo;of all the various tech fields, data science currently ranks the lowest in diversity\u0026rdquo;\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e. It\u0026rsquo;s not difficult to imagine that things are not much better for data science teams in large enterprises such as banks, telcos and retailers. With all the caveats associated with online surveys, the data suggests that only about 20% of data scientists across industries are women\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e","title":"Three Kinds of Diversity"},{"content":"Introduction In this three-part series we discussed the challenges that enterprises face when it comes to adopting AI \u0026ndash; especially those organizations that have traditionally not been in the business of software engineering:\nIdentifying the right opportunities (Part I) Product development (Part II) Avoiding common pitfalls This final part will conclude the series by going over a few common pitfalls of AI product development and how to avoid them. This list is by no means exhaustive and focuses on business and organizational pitfalls1 rather than those related to AI theory and technology.\nAvoiding Common AI Pitfalls Get a business sponsor AI projects are by their very nature complex and show value only slowly. In this regard they are more akin to R\u0026amp;D or innovation-type projects that require an initial investment without immediate payback. It\u0026rsquo;s thus all the more important to secure strong support from the side of the business that is supposed to benefit from (and often funds) the work.\nA good proportion of data science teams struggle, because they have entered an antagonistic, one-sided relationship with the business where they are seen as service providers. As a result, these teams loose their ability to identify with the business\u0026rsquo; goals and start to look inward, focusing on technical challenges instead of driving business value.\nGetting a business sponsors who is invested into the team\u0026rsquo;s success can help mitigate these risks. A sponsor ensures that the business relationship is collaborative and that goals are shared across teams. Developing an effective relationship with business stakeholders can be difficult2 and data science teams can benefit from someone who can liaise between them and the stakeholders. Finally, a sponsor can also help support project funding discussions by communicating timelines and risks to decision makers.\nStart simple Once a business sponsor has been found it is time to define the scope of the project. Data scientists and ML engineers are an especially ambitious bunch \u0026ndash; which is great \u0026ndash; but often associated with a tendency to overengineer and overachieve. Especially at the start of a project it is expedient to scale back the aspirations of the team and focus on what value can be delivered quickly with a reasonable amount of effort.\nThis initial, imperfect solution can then be used as a springboard to get the buy-in to develop a more sophisticated product. The added benefit is that user feedback can be collected. The question of whether an AI product provides value to the user or customer carries the largest risk and can often be answered without taking the most sophisticated AI approach.\nIt takes discipline and experience to identify the business problems that have the right complexity and don\u0026rsquo;t require unreasonable time and effort before value is measurable. But a good guideline is to halve the complexity of the AI solution that was initially proposed. In many enterprises ample value can be delivered even after radically scaling back the scope, especially when this is the first attempt of rolling out AI or machine learning at scale for a particular business unit.\nStart with augmentation over automation Augmentation and human-in-the-loop solutions are a good example of simplifying AI product design during the initial roll-out. Although full automation might be the end goal, a transitional augmentation phase has many advantages.\nAugmentation can be an effective way to incrementally work towards automation. It often takes time to narrow down the exact problem to solve with AI. Individuals who used to complete a task manually can help capture all the implicit assumptions and business rules when they are part of the augmentation phase.\nAugmentation can also speed up time-to-market. An AI application can already start generating value by augmenting the human user on the easiest-to-automate tasks. The experience can demonstrate whether it is worth automating the entire workflow.\nSometimes augmentation can free up enough time3 for humans to focus on tasks where we\u0026rsquo;re still superior to machines. In the end, this approach can find the sweet spot between augmentation and automation, which would have been overlooked by attempting full automation from the get-go.\nOf course, augmentation can also help minimize risk where an error may compromise human safety or increase financial risk. The benefit of fully automating a task may not outweigh the cost of the AI making occasional mistakes.\nIn summary, aiming for augmentation first can help de-risk AI projects, especially in enterprises with low AI maturity.\nDon\u0026rsquo;t get distracted by data This final AI pitfall refers the lack of data access or availability that hobbles so many projects in the beginning. This is especially frustrating for data scientists who joined the project to work on machine learning models and want to get started as quickly as possible.\nWhile the reasons for a lack of data can be manifold, they often shift focus away from the business problem. Instead, time is spent deliberating with data management, data governance, or IT teams on how to gain access to already existing data. Or plans have to be made to collect or buy new data.\nAI products need to straddle data requirements, model insights and user experience. Focusing efforts on one corner of the triangle, e.g. data, can risk falling behind on modelling and user experience.\nThis is not to say that data isn\u0026rsquo;t important, but time spent on data availability is time not spent on working toward a great user experience. Data, model, and user experience are interdependent and cannot be solved independently. Spending time on data alone risks acquiring the wrong data or creating data infrastructure that is incompatible with the use case. Data lakes, which enterprises spend years filling, are a good example. AI product development is sometimes delayed by these projects and often requires significant adjustments to data and data infrastructure once started.\nIt should not be forgotten that building models and exposing them to the user can help inform the data acquisition process. Many of the questions around volume, state, and predictiveness of the data can only be answered by starting the modelling process. The question of whether the model‚Äôs insights are needed, useful, and actionable can only be answered once the user is exposed to them.\nConclusion Not every team or project will encounter the same AI product development pitfalls. Many problems can be avoided with experience, foresight, and adherence to best practices. But these pitfalls also stem in part from patterns that bias teams to behave in certain ways.\nTeams might be reluctant to interact with the business to get a sponsor, because these are usually people from a different background, who speak a different language. Teams might be ambitious, thinking they can or should solve the challenge by themselves. It\u0026rsquo;s easy to get enamoured with new, shiny frameworks and start designing solutions that are too complex and built on technology that people have little experience with.\nSo the journey to embrace best practices is also always a fight against the forces that pull us away from adopting them.\nRead about \u0026ldquo;how to identify AI opportunities\u0026rdquo; in Part I and \u0026ldquo;AI product development\u0026rdquo; in Part II of this series.\nHow AI Fits into Your Data Science Team\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHow to Work With Stakeholders as a Data Scientist\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf done right, this will also provide a way to introduce AI more gracefully without threatening to displace someone\u0026rsquo;s job.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-07-30-ai-product-development-part3/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn this three-part series we discussed the challenges that enterprises face when it comes to adopting AI \u0026ndash; especially those organizations that have traditionally not been in the business of software engineering:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIdentifying the right opportunities (\u003ca href=\"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/\"\u003ePart I\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eProduct development (\u003ca href=\"https://www.talesofindustry.org/post/2022-06-01-ai-product-development-part2/\"\u003ePart II\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eAvoiding common pitfalls\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis final part will conclude the series by going over a few common pitfalls of AI product development and how to avoid them. This list is by no means exhaustive and focuses on business and organizational pitfalls\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e rather than those related to AI theory and technology.\u003c/p\u003e","title":"AI Product Development (Part III)"},{"content":"Introduction Enterprises face three broad challenges when it comes to adopting AI. These are especially relevant to organizations that have traditionally not been in the business of software engineering and AI:\nIdentifying the right opportunities (Part I) Product development Avoiding common pitfalls An approach to identifying the right opportunities \u0026ndash; the Double Diamond \u0026ndash; was presented in Part I of this three-part series. Here we will focus on the second challenge: Product development; or how to create an environment where successful AI products can be built confidently.\nAI Product Development Once the right problem has been identified, it\u0026rsquo;s time to build the solution. Easier said than done for organizations that have not been set up to engineer software products, let alone AI products. Even companies that are software engineering companies at heart, are not insulated from failure and have a long list of abandoned products. See the Google Graveyard, for example.\nBut AI product development does not have to be a source of frustration when a few principles are taken into account. These are well-understood in the software engineering industry. I\u0026rsquo;ll highlight a few of them below and how they apply to AI development specifically.\n1. People It\u0026rsquo;s alarming that even to this day some organizations try to \u0026ldquo;save\u0026rdquo; money by not adequately staffing what is essentially a software engineering project. Every little dev shop has by now figured out that building quality products requires a project manager, Scrum master, designers, developers, and QA engineers. AI products require additional resources, such as data engineers, data scientists, machine learning engineers, etc.\nA complete AI product team. Business stakeholders in ties. (Public Domain)\nAssuming that a team of data scientists is able to deliver robust software products continues to be the reason for many failed or massively delayed projects. It\u0026rsquo;s paramount to hire the roles required to build products, not just models.\nHigh-performance teams can do amazing work during the experimentation and proof-of-concept phase, but even they will reach their limit if you ask them to scale out a solution without the dedicated resources necessary to build and maintain robust cloud architecture.\nSome organizations find it challenging to attract this kind of talent. In those cases, start by investing in \u0026ldquo;seed\u0026rdquo; talent, i.e. people who have a network, perhaps in the start-up world. You should aim for at least 20-30% of highly skilled people in the early days of your organization\u0026rsquo;s AI transformation. These people can spearhead the changes necessary to adopt modern engineering practices and demonstrate how it\u0026rsquo;s done by example. Don\u0026rsquo;t forget to develop a talent strategy with your talent/HR team that allows you to attract and retain the right kind of talent.\nAll of this doesn\u0026rsquo;t happen overnight. It\u0026rsquo;s not unusual for organizations to make multiple attempts to build the right talent pool. Sometimes they oscillate between \u0026ldquo;all-in\u0026rdquo; (hire the A-Team) and \u0026ldquo;back-to-basics\u0026rdquo; (let IT do it) before they converge on a better understanding of their AI talent needs.\nSuffice to say, the talent an organization is able to attract and retain defines what can be achieved. An incomplete product team will build incomplete products.\n2. Process AI products are built in the context of an organization. Any development effort is reliant on the processes that govern the collaboration across teams and the access to the right tools and infrastructure.\nAgile and Scrum are widely know and will not be discussed here. However, there are two processes that are essential to AI product development and somewhat independent of agile: enablement via the right tools and discovery vs engineering.\nEnablement via the right tools Data scientists and machine learning engineers need the right tools to succeed. Nobody would equip a professional cycling team with city bikes, because that\u0026rsquo;s all they need to cross the finishing line.\nIt\u0026rsquo;s a sad fact of this industry that not all data science teams have access to adequate version control systems, developer-friendly computers, open source tools, live data or cloud infrastructure. These are very basic preconditions of success and an organization is undermining itself if talent is handicapped this way. It\u0026rsquo;s also disrespecting to not give someone the right tools to succeed at their job. Top talent will leave and an organization\u0026rsquo;s reputation as an attractive employer for AI talent will suffer.\nJan and Huub are trying out their new MacBooks. (Nationaal Archief)\nBeyond basic enablement, data science teams should use the tools that they can master and have a plan to learn the tools that are required. In practice that means staying away (at first) from frameworks that are overkill or beyond the team\u0026rsquo;s ability, like, for example, Kubernetes. On the flip side, teams that are working exclusively on Jupyter notebooks need to develop a plan to adopt the tools for building production-ready code, e.g. packaging and GitLab CI/CD.\nHaving a technology adoption strategy in a fast-moving ecosystem like AI is key. This includes giving the team the time to learn required tools and evaluate promising tools. The hiring process should be synchronized with the technology strategy to ensure that new hires have practical experience where needed and can possibly teach the team new tools. However, technology is there to enable teams and shouldn\u0026rsquo;t become a distraction or worse an end in itself.\nDiscovery vs Product Engineering Building a solution that derives insights from data requires two fundamental types of activities: discovery and product engineering1. The distinction is necessary, because the data often affects the solution design. Put differently, serious engineering cannot commence before the data (and therefore the opportunity) is sufficiently understood.\nLess AI-mature enterprises often get these phases mixed up and advances in discovery are sometimes viewed as product development progress. Business stakeholders can get frustrated when they are presented with discovery findings, but then have to wait a long time to see the same results in a fully scaled-out application. Similarly, data science teams can get caught up in an infinite discovery cycle where new findings beget further rounds of discovery.\nIt\u0026rsquo;s thus important to mitigate the risk of discovery by properly scoping and time-boxing the activity with the goal of flowing the findings into the final product. At the same time, any engineering activity should not be rushed until there\u0026rsquo;s evidence in the data that the activity if valuable.\nFor example, building a forecasting engine requires testing the hypothesis that a prediction of sufficient accuracy can be made. The forecasting accuracy can be evaluated during the discovery phase without investing into a scaled out forecasting engine. The next step is then to eliminate the risk of technical feasibility, i.e. can the forecasting be run across all data at the desired frequency. This phase should be focused on the implementation and not be derailed by further feature enhancements to the model. The model\u0026rsquo;s feature enhancements can then be addressed in the next round of discovery and product engineering.\nThere are frameworks, such as CRISP-DM2, that formalize the transition between different phases during the iterative development of AI products. It\u0026rsquo;s helpful to adopt a process that facilitates the flow of discovery findings into the AI product to establish a healthy cadence of product advancement.\nConclusion The right team and processes are instrumental to an organization\u0026rsquo;s success when it comes to building AI products. Although most teams have to operate under resource constraints, it\u0026rsquo;s still worthwhile to develop a talent and a technology enablement strategy. These strategies make sure that the available resources are invested where they matter most.\nWe will address the final challenge to AI adoption (\u0026ldquo;avoiding common pitfalls\u0026rdquo;) in the upcoming Part III of this series. Read about \u0026ldquo;how to identify AI opportunities\u0026rdquo; in Part I.\nData Science vs Software Engineering\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCRoss Industry Standard Process for Data Mining (CRISP-DM).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-06-01-ai-product-development-part2/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eEnterprises face three broad challenges when it comes to adopting AI. These are especially relevant to organizations that have traditionally not been in the business of software engineering and AI:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIdentifying the right opportunities (\u003ca href=\"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/\"\u003ePart I\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eProduct development\u003c/li\u003e\n\u003cli\u003eAvoiding common pitfalls\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn approach to identifying the right opportunities \u0026ndash; the Double Diamond \u0026ndash; was presented in \u003ca href=\"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/\"\u003ePart I\u003c/a\u003e of this three-part series. Here we will focus on the second challenge: \u003cem\u003eProduct development\u003c/em\u003e; or how to create an environment where successful AI products can be built confidently.\u003c/p\u003e","title":"AI Product Development (Part II)"},{"content":"Introduction Depending on how you count, \u0026ldquo;legacy\u0026rdquo; enterprises have tried to adopt modern machine learning for close to a decade. And while some organizations have been successful, there are still large swaths of companies that are fighting to even automate their internal reporting. At the same time, data science training is widely available and lots of machine learning tools have matured and are freely available as open source. So why have some organizations barely started to leverage AI?\nNon-digitally native enterprises often struggle with three broad challenges when it comes to creating AI-powered products.\nIdentifying the right opportunities Product development Avoiding common pitfalls Identifying AI Opportunities The news are awash with the most extraordinary advances in AI and machine learning. AI can write entire essays to answer questions1, generate convincing portraits2, and create pictures according to a description3.\nIronically, all that does is to set the expectations for AI much too high for your typical enterprise. Companies such as Amazon, Google, Microsoft, Apple, and Meta spend tens of billions of dollars on R\u0026amp;D4. Don\u0026rsquo;t expect the same outcome with any less commitment!\nIn any case, identifying AI opportunities in an enterprise context is often approached backwards. Starting with a particular ML approach (no matter how exciting) and then trying to find problems that can be solved by it, is a sure-fire way to solving the wrong problems.\nEnterprises need to follow a human-centered approach that starts with understanding user needs and see AI as a tool, a very powerful one, that allows us to imagine further than we ever thought possible.\nThe Double Diamond Framework The Double Diamond Framework puts the observation and discovery of user needs at the start of the problem-solving process. There are various alternative Design Thinking approaches, all of which try to help answer the two most important questions of product ideation and development.\n1. What is the right _problem_ to solve? 2. What is the right _way_ to solve it? These two questions correspond to the two diamonds of the Double Diamond Framework. The goal of the first diamond is to ensure that we are solving the right problem. We do this by uncovering the core challenges faced by users and translating these insights into ideas during the divergent discovery phase. All ideas are then filtered to ensure that they are desirable and feasible within the project context during the convergent definition phase. Once the problem has been well defined, we move to the second diamond.\nThe Double Diamond highlights the two stages of Design Thinking and the alternation between divergent and convergent thinking.\nThe goal of the second diamond is to solve the problem the right way. We do this by designing and building the experience often in form of a prototype or MVP during the development phase. Any assumptions can be verified quickly based on the data gathered via a solid build-measure-learn process. The final delivery phase is then all about delivering a viable solution to the user or customer.\nThe Double Diamond framework has been described in more detail here5 and here6. It might, however, be worth highlighting how it applies to AI specifically.\nThe table summarizes some key activities and outcomes of Double Diamond\u0026rsquo;s four phases. These might look familiar to anyone who has used this approach before to build a software product. The \u0026ldquo;AI Awareness\u0026rdquo; column highlights the questions that can be asked by the product designer and members of the AI team to identify AI opportunities. They should be asked with humility and not with the aim to shoehorn AI into the final solution.\nStep Activities Outcomes AI Awareness 1. Explore and Learn User interviews Existing experience journey map Synthesize and prioritize User needs across social, emotional and functional dimensions Pain points across the end-to-end user journey Is the task repetitive? Is work offshored/ outsourced? Is the quality of work variable? 2. Ideate and Define Future state journey map \u0026amp; ideation Low fidelity designs Product vision, objectives, and measures of success Product roadmap What if we could automate X? What if we could support task X with predictions and insights? 3. Design and Develop High fidelity design Prototyping \u0026amp; technical implementation Build and launching a product or feature Consider starting with few/ simulated data Test model output with users early 4. Evaluate and Evolve Measure user behaviour Scale \u0026amp; and stabilize Validate hypothesis Determine whether pivot or persevere Continue to learn from user feedback Monitor model performance \u0026amp; address model drift \u0026copy; 2022 talesofindustry.org Being aware of the tremendous capabilities that AI provides can lead to better ideas. For example, we can consider AI whenever we come across users who are frustrated by having to engage in tedious or repetitive tasks. AI is well-suited for automating tasks that require insights derived from unstructured data, such as images and text.\nAgain, AI should not be the starting point. The design process starts with the user and their frustrations, and then devise ideas to solve the problem. These ideas might or might not include AI as a tool to engineer the better solution.\nConclusion A formalized process, such as the Double Diamond, has been proven useful across the industry and can be adapted to include AI as discussed. If your organization is struggling to identify opportunities and deliver on them, you can try to adopt this approach. Depending on the maturity level of your organization or team, it might also be worthwhile hiring a specialist who can provide training and advice. There\u0026rsquo;s no need to \u0026ldquo;reinvent the wheel\u0026rdquo;. Taking a human-centered approach by starting with user needs and then thinking about how AI can address those needs is a powerful recipe for building experiences users love7.\nWe will address the two remaining challenges to AI adoption (\u0026ldquo;product development\u0026rdquo; and \u0026ldquo;avoiding common pitfalls\u0026rdquo;) in the upcoming Part II of this series.\nGPT-3 Powers the Next Generation of Apps\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlias-Free Generative Adversarial Networks (StyleGAN3)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDALL¬∑E: Creating Images from Text\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGoogle Needs To Make Machine Learning Their Growth Fuel\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhat is the framework for innovation?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDesign Thinking 101 ‚Äî What Is It? (Part I), (Part II)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis article is based on a version Aneesh Datta and I previously published on Rangle.IO\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-05-08-ai-product-development-part1/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eDepending on how you count, \u0026ldquo;legacy\u0026rdquo; enterprises have tried to adopt modern machine learning for close to a decade. And while some organizations have been successful, there are still large swaths of companies that are fighting to even automate their internal reporting. At the same time, data science training is widely available and lots of machine learning tools have matured and are freely available as open source. So why have some organizations barely started to leverage AI?\u003c/p\u003e","title":"AI Product Development (Part I)"},{"content":"Getting Started with Hugo Hugo is a static site generator (SSG). Simply put, an SSG prepares everything needed to display a website. In contrast, dynamically assembled websites obtain their assets only after they get accessed. An SSG thus minimizes the computational demand on the server, which allows providers, such as GitHub Pages and others1, to host your static sites for free. More importantly, SSGs emanate a philosophy of simplicity-by-design stemming from a number of limitations. Being constraint forces content creators to focus on the content itself and minimizes the distractions of layout and design. SSGs are thus especially suited for blogs, landing pages, documentation and portfolio sites.\nThe following is based on Ryan Schachte\u0026rsquo;s excellent video2 and similar tutorials (see links below).\nInstall Hugo On MacOS install Hugo from the command line using the Homebrew package manager.\nbrew install hugo On Debian Linux install Hugo with:\nsudo apt-get install hugo More installation instructions can be found here.\nSetting Up Two Repositories We will use GitHub Pages to host our static site for free. GitHub Pages are easily set up by creating a new public repository named username.github.io, where username is your GitHub username. If you were do save a simple HTML page as index.html in such a repo, you\u0026rsquo;d be able to view it at https://username.github.io.\nHowever, we\u0026rsquo;ll be slightly more considerate than dumping all of Hugo\u0026rsquo;s configuration code, templates, themes and markdown content into the GitHub repository. After all, to display the website correctly only the static site assets that Hugo compiles are needed. We are going to track all the rest in a separate repository called \u0026ldquo;blog\u0026rdquo;.\nSo please head over to GitHub and create two repositories:\nusername.github.io: A public repository for the static site assets. blog (can have a different name): A private repository for Hugo containing configurations, themes, and unpublished posts. Setting up Hugo Once we\u0026rsquo;ve created two repositories, we\u0026rsquo;ll clone them on our local system (git clone git@github.com:username/blog.git) and enter the \u0026ldquo;blog\u0026rdquo; repository (cd blog).\nHere we create the scaffolding for our website. In this case Hugo will create a new directory hugo-blog that contains all the Hugo assets.\nhugo new site hugo-blog Adding a Theme Enter the hugo-blog/themes directory and clone your preferred Hugo theme. For example, Hugo Bear Blog from Jan Raasch. Click on the \u0026ldquo;Download\u0026rdquo; button, copy the repository URL, and clone the theme into your themes/ folder using the submodule action. The submodule action avoids problems arising from nested cloning3.\n# Option 1: Original theme repo git submodule add https://github.com/janraasch/hugo-bearblog.git themes/hugo-bearblog Alternatively, you can first fork the theme directory by going to GitHub and clicking \u0026lsquo;Fork\u0026rsquo;. This has the advantage of being able to push changes to your fork of the themes directory later on.\nDon\u0026rsquo;t forget to add the fork as a submodule instead of the original repo. Replace username and hugo-theme with the respective values.\n# Option 2: Forked theme repo git submodule add https://github.com/username/hugo-theme.git themes/hugo-theme Then we add the theme\u0026rsquo;s name to the config.toml in blog\u0026rsquo;s base directory, e.g. hugo-blog/. The file might look like this:\nbaseURL = \u0026#39;https://username.github.io/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My Blog Title\u0026#39; theme = \u0026#39;hugo-bearblog\u0026#39; The values of baseURL and theme obviously depend on your choice of host and theme.\nAdding the static site content repo as a submodule As mentioned above, we are decided to create a separate repository for the compiled static site content. In essence, we will let the username.github.io repository only track Hugo\u0026rsquo;s public/ output directory and nothing else. This way the commit history will purely reflect content updates. Plus, we can track any unpublished content in a private repo hidden from public view. The downside is that we have to remember to update both repositories.\nRun the following command from within the blog\u0026rsquo;s base directory (e.g. hugo-blog/) to link your public GitHub Pages repo to the public/ directory of your private blog repo. (If you receive an \u0026lsquo;already exists\u0026rsquo; error you might have to remove the public/ directory or its contents first.)\ngit submodule add -b main https://github.com/username/username.github.io.git public Building the site To perform the site build, run the following commands\n# create static assets in the \u0026#39;public/\u0026#39; directory hugo # commit the changes to the public submodule repository cd public git add . git commit -m \u0026#34;initial build\u0026#34; # commit references to submodule changes cd .. git add . git commit -m \u0026#34;initial build - update submodule references\u0026#34; # push both the source project and the submodule to remote git push -u origin main --recurse-submodules=on-demand Remember these command, you\u0026rsquo;ll have to repeat them for most changes to the site.\nThis concludes the basic Hugo setup. In the following we\u0026rsquo;ll go through posting, theme customization, adding images, and analytics. Let\u0026rsquo;s create our first post.\nCreating a Post Hugo posts are created in Markdown. The limited formatting options force authors to focus on the content. In general, it\u0026rsquo;s good advice not to try to change the formatting beyond what markdown offers. The theme\u0026rsquo;s template and CSS files provide better options while ensuring that the layout stays consistent across the site.\nTo create a new post, run:\nhugo new blog/mypost.md This will create a markdown file in the content/ directory, e.g. content/blog/mypost.md. Depending on your theme, you might have to substitute blog/ with post/ or posts/. Consult your theme\u0026rsquo;s documentation.\nOpen the freshly created post with your favorite editor. Hugo posts start with a preamble, the key: value pairs between the lines (---). The preamble sets various variables that Hugo will use to compile the site. Depending on the theme, you can add cover images, tags, and category labels. The draft status and publication date affect whether a post is published. You can add content below the preamble, like so.\n--- title: \u0026#34;My First Post\u0026#34; date: 2020-01-01T12:15:00+01:00 draft: true --- # Introduction Some **bold** text and a [link](example.com). Now fire up the Hugo server locally in \u0026lsquo;draft\u0026rsquo; mode and open the respective URL with your browser, e.g. http://localhost:1313.\nhugo server -D If you\u0026rsquo;re new to Hugo, check out a particular theme\u0026rsquo;s example content. Not every theme will display posts the way you might expect. If you get stuck, start with the theme\u0026rsquo;s configuration files (config.toml) and content directories. The Bearblog example site is a good start if you decided to use this theme.\nDrafts and Future Posts Note that the server doesn\u0026rsquo;t show blog posts with draft status by default. You can either set draft: true in the markdown file\u0026rsquo;s preamble or run the hugo server in draft mode. Besides draft status there are two more conditions that might prevent a post from being published, i.e. posts with a publication date in the future and posts with an expiry date4.\n# compile posts with draft status hugo server -D # compile posts with a future publication date hugo server -F Publishing To publish your site, run Hugo excluding drafts and future posts and push the changes to the remote repositories as described above.\nTracking Visitors Hugo and various themes make it easy to add analytics. Start by creating a new account or sign in with an existing Google account at https://analytics.google.com/.\nThen set up your \u0026ldquo;Property\u0026rdquo;, give it a name, and point it to the URL of the site you plan on tracking. Finally, click through the basic options until you land on a page with a Tracking Code which might look something like this: G-XXXXXXXXXX.\nEdit Hugo\u0026rsquo;s configuration file, e.g. config.toml, and add the tracking code.\ngoogleAnalytics = \u0026#39;G-XXXXXXXXXX\u0026#39; You should now be able to track visitors after publishing these changes. Then click on \u0026lsquo;Reports\u0026rsquo; and then on \u0026lsquo;Realtime\u0026rsquo; on https://analytics.google.com/ to track yourself visiting the page. This should work even if you run a server on your own machine and browse the site locally.\nCustomization Hugo provides endless possibilities for customization. Virtually everything can be adjusted, including spacing, font sizes, colors, text alignment, and content interaction. The basic principle is to \u0026lsquo;override\u0026rsquo; a theme\u0026rsquo;s default templates. It\u0026rsquo;s relatively easy to use a browser\u0026rsquo;s \u0026lsquo;inspection\u0026rsquo; tools, click on the ‚åñ , and hover over the element that you want to adjust. This should help you understand what CSS properties are being applied to this element. You might also be able to glean a few keywords that you can later use to search for.\nModifying a Theme with Overrides Most theme modifications can be done without modifying the theme\u0026rsquo;s source code repository. Hugo allows overwriting template files with your own modified version that you keep in a separate directory5.\nFor example, it\u0026rsquo;s relatively straight-forward to modify the copyright footer because most themes keep its template in a separate file. Let\u0026rsquo;s assume the footer template is kept in\nthemes/\u0026lt;THEME\u0026gt;/layouts/\u0026lt;SUBDIR\u0026gt;/footer.html Obviously, the \u0026lt;THEME\u0026gt; and \u0026lt;SUBDIR\u0026gt; directories depend on your particular theme. If you have trouble finding the right file, browse your theme\u0026rsquo;s layouts/_default directory or grep for a keyword, e.g. grep footer -R. That should give you a rough idea of where to look.\nYou can override any theme file by placing a matching file in your main site‚Äôs directory, e.g.:\nlayouts/\u0026lt;SUBDIR\u0026gt;/footer.html You might have to create the directory structure first, e.g. mkdir -p layouts/\u0026lt;SUBDIR\u0026gt; before you copy the template with cp -iv themes/\u0026lt;THEME\u0026gt;/layouts/\u0026lt;SUBDIR\u0026gt;/footer.html layouts/\u0026lt;SUBDIR\u0026gt;/\nThis copy has precedence over the theme\u0026rsquo;s \u0026lsquo;default\u0026rsquo; templates. You can edit the copy to your heart\u0026rsquo;s content. The changes should be immediately visible (or any errors) if you\u0026rsquo;re running the server (hugo server) at the same time. Reverting back to the original theme is as easy as deleting the file.\nOverride CSS To override the theme\u0026rsquo;s style sheet, consult its documentation. Most themes support placing one or several CSS files in the assets directory.\nassets/css/extended/custom.css Images Images help your site stand out. They are added to the static/ directory or any subdirectory within. For example, after creating a subdirectory (mkdir -p static/images/), images can then be referenced in markdown.\n![Alt Description](/images/cover.jpeg) Alternatively, some themes allow adding a caption (i.e. title) by using the figure shortcode.\n{{\u0026lt; figure src=\u0026#34;/images/cover.jpeg\u0026#34; title=\u0026#34;Caption text.\u0026#34; \u0026gt;}} Image width and alignment can be adjusted using this shortcode.\n{{\u0026lt; figure src=\u0026#34;/images/cover.jpeg\u0026#34; caption=\u0026#34;Caption text.\u0026#34; align=\u0026#34;center\u0026#34; width=\u0026#34;600px\u0026#34; \u0026gt;}} Math Typesetting, Equations and LaTeX Unfortunately, Hugo does not support math typesetting out of the box (2025-01-20). However, it is possible to add math support with a few modifications. Before proceeding, consult your theme\u0026rsquo;s documentation as the exact steps might depend on the particular theme.\nFirst, we need to make sure that every site that needs to render math has the required scripts and stylesheets added to its header. Here we use KaTeX, but MathJax and others are also available.\nReplace the respective version numbers (@0.16.21) with the latest and save the following chunk as layouts/partials/helpers/katex.html. (Don\u0026rsquo;t modify the theme\u0026rsquo;s folder, i.e. themes/\u0026lt;THEME\u0026gt;/layout/!)\n\u0026lt;!-- Include external KaTeX resources for math typesetting. Set version numbers to latest. --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css\u0026#34; /\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js\u0026#34; onload=\u0026#34;renderMathInElement(document.body);\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- Add support for rendering inline math using a single \u0026#39;$\u0026#39;. --\u0026gt; \u0026lt;script\u0026gt; document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function () { renderMathInElement(document.body, { delimiters: [ { left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true }, { left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false }, ], }); }); \u0026lt;/script\u0026gt; The second step is to get the theme\u0026rsquo;s template to include this chunk of code in the header of a page. Search for where your theme defines the \u0026ldquo;partials\u0026rdquo; to include in the header6.\nFor example, the Papermod theme already provides a mechanism to add custom code to the header by placing a file in the \u0026ldquo;override\u0026rdquo; layout directory (layout/partials/). We can thus avoid modifying the theme folder (themes/\u0026lt;THEME\u0026gt;/layout/partial/), which allows us to keep updating it with git.\nSave the following code as layouts/partials/extend_head.html. Use partialCached for faster rendering.\n\u0026lt;!-- extend_head.html --\u0026gt; ... {{ if .Params.math }}{{ partial \u0026#34;helpers/katex.html\u0026#34; . }}{{ end }} ... Finally, test that the math typesetting works by creating a post and running hugo server -D. Don\u0026rsquo;t forget to set math: true in the front matter (i.e. markdown header).\n--- title: \u0026#34;DRAFT: Math Typesetting\u0026#34; draft: true math: true --- Inline math equations, like $c = \\sqrt{\\frac{E}{m}}$ are cool. The block equation below is cool, too: $$ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$ The above should be displayed like this:\nInline math equations, like $c = \\sqrt{\\frac{E}{m}}$ are cool. The block equation below is cool, too:\n$$ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\nConsult the KaTeX reference for supported function.\nInteractive Data Visualization with Vega Vega-Lite makes it quite easy to add good-looking, interactive graphs to your blog. The Altair library offers a convenient Python API to generate Vega-Lite-compatible plots. (Install it with e.g. pip install altair).\nBoth, the plot specifications and the data are saved to a JSON file. The plot can then be rendered using the JavaScript function vegaEmbed(). Having the raw data embedded in the JSON file allows interactive plots to filter and further process the data at the behest of the user; see these examples.\nNote, that storing data as JSON text is quite inefficient and displaying embedded data with more than 5000 rows might require a different approach.\nTo enable Hugo to display Vega plots, the required Vega assets need to be loaded. First, create a \u0026ldquo;partial\u0026rdquo; in your layout folder, e.g. layouts/partials/vega.html. Make sure to update the @-version numbers to the latest releases.\n\u0026lt;!--https://vega.github.io/vega-lite/usage/embed.html--\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/vega@5.30.0\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/vega-lite@5.21.0\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/vega-embed@6.26.0\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Second, enable the conditional import in the site\u0026rsquo;s header by creating or modifying layouts/partials/extend_head.html6.\n{{ if or .Params.vega }} {{ partial \u0026#34;vega.html\u0026#34; . }} {{ end }} This avoids loading the Vega assets unless the Vega option is set to vega: true in the respective page\u0026rsquo;s front matter.\nThird, create a {{\u0026lt; vegaembed \u0026gt;}} shortcode (e.g. layouts/shortcodes/vegaembed.html) to be able to place a plot in markdown.\n\u0026lt;div id=\u0026#39;{{ .Get \u0026#34;id\u0026#34; }}\u0026#39;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; var spec = \u0026#39;{{ .Get \u0026#34;spec\u0026#34; }}\u0026#39;; vegaEmbed(\u0026#39;#{{ .Get \u0026#34;id\u0026#34; }}\u0026#39;, spec) .then(function (result) {}) .catch(console.error); \u0026lt;/script\u0026gt; Finally, in the markdown file use the vegaembed shortcode to specify the path to the Vega JSON file and a unique id=.\n--- title: \u0026#34;DRAFT: Math Typesetting\u0026#34; draft: true vega: true --- # Title {{\u0026lt; vegaembed id=\u0026#34;vis1\u0026#34; spec=\u0026#34;/graphs/bar-chart1.vg.json\u0026#34; \u0026gt;}} ... If the plot is not showing, check that the id\u0026rsquo;s are unique, the JSON files are accessible and their paths are correct.\nSite Bundles \u0026amp; Multi-File Posts Posts can be put into their own directory together with all their assets. The main advantage is that all post-specific assets are kept in one place. This way markdown content can be split across multiple files which improves the readability of the main file by keeping, for example, long code blocks in separate files.\nSplitting markdown content across multiple files and importing it into the main file doesn\u0026rsquo;t seem to be supported out of the box. We thus need to create a shortcode:\n\u0026lt;!-- layouts/shortcodes/include.html --\u0026gt; {{ $file := .Get 0 }} {{ (printf \u0026#34;%s%s\u0026#34; .Page.File.Dir $file) | readFile | markdownify }} There are many variants of this shortcode posted online, e.g. here and here. I found that markdownify (instead of safeHTML) is the only way to ensure that code, math, and shortcodes are rendered correctly. The complicated-looking (printf \u0026quot;%s%s\u0026quot; .Page.File.Dir $file) enables importing files that are located in the same \u0026ldquo;bundle\u0026rdquo; directory by name without having to specify their absolute file path, e.g. content/post/....\nThen create your bundle directory (e.g. content/post/mypost) and place the main markdown file, which needs to be called index.md at the root of it. You can now import additional content saved to markdown files in the bundle directory with the \u0026ldquo;include\u0026rdquo; shortcode.\n\u0026lt;!-- content/post/mypost/index.md --\u0026gt; # Appendix Some text... {{% include \u0026#34;content-1.md\u0026#34; %}} ... Block Crawlers with robots.txt Do you want to protect your content from automated non-consensual theft or limit the bandwidth occupied by crawlers? The latter should be less of a concern, because Hugo\u0026rsquo;s static sites are extremely efficient as opposed to dynamically-generated sites.\nYou might, however, want to prevent commercial services from monetizing your content via ad-sponsored search or AI-subscription services. AI models trained on your content might be able to reproduce your work pretty faithfully without attribution, or even generate things based on your content.\nUnfortunately, you have to rely on the honor system. Unless your site requires a login, all publicly published information can be crawled and copied. Your first line of defense is the robots.txt file at the root of your site. It is automatically generated if you set the enableRobotsTXT option in your config file to \u0026ldquo;true\u0026rdquo;.\nFirst, copy the existing Hugo robots.txt template from your theme\u0026rsquo;s layout folder to your own and edit it there. The basic idea is to explicitly disallow specific user agents (i.e. crawlers):\nUser-agent: GPTBot Disallow: / User-agent: Google-Extended Disallow: / ... This approach allows your site to still be indexed for search, e.g. by Googlebot, while hopefully keeping AI crawlers out. As of this writing (January 2025), there are at least six AI crawlers: CCBot, GPTBot, ChatGPT-User, Google-Extended, FacebookBot, Omgilibot. Find more information here.\nBesides robots.txt you can add meta tags to each page\u0026rsquo;s header and ask not to follow links.\n\u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Example Webpage\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;This is an example webpage\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;example, webpage\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;John Doe\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;robots\u0026#34; value=\u0026#34;noindex\u0026#34; /\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;a href=\u0026#34;‚Ä¶\u0026#34; rel=\u0026#34;nofollow\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/body\u0026gt; Again, you would have to edit templates or create partials to achieve this.\nCustom Domain Serving your site from a different domain than username.github.io is relatively straightforward7. You can buy a domain at any registrar, e.g. Google Domains.\nThen go to \u0026lsquo;Pages\u0026rsquo; section within the \u0026lsquo;Settings\u0026rsquo; of your GitHub Pages repository. Here you can add your domain. Both, apex domain (example.com) or a subdomain (www.example.com or blog.example.com) are possible.\nThen link your domain to GitHub Pages8. On Google Domains, your DNS resource records should looks like the following table with example.com replaced by the domain you own and username replaced with your GitHub Pages name.\nHost Name Type TTL Data example.com A 1 hour 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 www.example.com CNAME 1 hour username.github.io For information and the latest IP addresses consult the GitHub documentation.\nIf everything is set up correctly and the record had enough time to propagate, you should see a green \u0026lsquo;‚úî DNS check successful\u0026rsquo; below the \u0026lsquo;Custom Domain\u0026rsquo; section in the GitHub Pages settings.\nDon\u0026rsquo;t forget to update the baseURL in your config.yaml to your custom domain, e.g. https://www.example.com/.\nSocial Media There are various ways to optimize the display and searchability of your page on social media. You can get an idea of how your site looks when shared on social media with Metatags or LinkedIn\u0026rsquo;s post inspector.\nWhat Next Join the Hugo forum. Conclusion We\u0026rsquo;ve barely scratched the surface of what Hugo can do. Setting up a basic static site is not as easy as some commercial offers like Squarespace and Wordpress. But if you\u0026rsquo;re comfortable on the command line and using an markdown editor this setup offers you full freedom over every aspect of your site. And it\u0026rsquo;s free!\nFAQ How do I update themes? If you\u0026rsquo;ve used the git submodule add command to clone a theme repository into the themes/ folder you can preview possible updates\ngit fetch origin git status or pull updates and merge them automatically\ngit pull In case you forked a theme, don\u0026rsquo;t forget to synchronize the fork before pulling the updates. More info here.\nI forgot to add \u0026ndash;recurse-submodules You might not see changes appear on GitHub Pages if you forgot to add the --recurse-submodules switch when pushing the main source repository. You can rectify this by entering the submodules directory (e.g. cd public) and running git push there manually.\nMy public folder is not clean The public folder tends to accumulte folders and files form drafts and upublished content when, for example, hugo -D is run. These can be removed with Hugo\u0026rsquo;s --cleanDestinationDir option. However, this might cause issues when the public/ folder is a git submodule.\nfatal: in unpopulated submodule If you can\u0026rsquo;t seem to be able to git add files in the public/ folder anymore, you might have to remove and add the public submodule again9. This might be an issue caused by running hugo --cleanDestinationDir.\ngit submodule deinit public rm -rv public git commit -m \u0026#34;remove folder public/\u0026#34; git push # add it back, add --force if necessary, do NOT run hugo yet! git submodule add -b main https://github.com/username/username.github.io.git public git add -u git commit -m \u0026#34;add submodule \u0026#39;public\u0026#39; back\u0026#34; git push My code doesn\u0026rsquo;t show up If you want to talk about literal Hugo code itself, you have to prevent the Hugo compiler from \u0026ldquo;interpolating\u0026rdquo; it. Comment-out the inside of double-bracketed code with /* and */.\nFor example, {{%/* shortcode */%}} will be displayed as {{% shortcode %}}, instead of executing the shortcode and replacing the double-bracketed expression with the result.\nLinks Static sites can be hosted in a number of locations, such as Amazon S3, Azure, CloudFront, DreamHost, Firebase, GitHub Pages, GitLab Pages, Google Cloud Storage, Heroku, Netlify, Surge.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCreating a Blog with Hugo and Github in 10 minutes\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvoiding Git Problems When Installing a Theme to Hugo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHugo: Draft, Future, and Expired Content\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCustomize a Theme\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor example, the Papermod theme defines the header in themes/\u0026lt;THEME\u0026gt;/layout/partial/baseof.html.\n\u0026lt;!-- baseof.html --\u0026gt; ... \u0026lt;head\u0026gt; {{- partial \u0026#34;head.html\u0026#34; . }} \u0026lt;/head\u0026gt; ... The head.html file in turn includes the extend_head.html \u0026ldquo;partial\u0026rdquo;.\n\u0026lt;!-- head.html --\u0026gt; ... {{- partial \u0026#34;extend_head.html\u0026#34; . -}} ... Creating this extend_head.html file in the local layout directory avoids touching any of the theme\u0026rsquo;s files.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAdding a Custom Domain to your Hugo Site on Github Pages.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLinking A Custom Domain To Github Pages\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee also this link.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2022-03-05-how-to-create-this-blog/","summary":"\u003ch1 id=\"getting-started-with-hugo\"\u003eGetting Started with Hugo\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e is a static site generator (SSG). Simply put, an SSG prepares everything needed to display a website. In contrast, dynamically assembled websites obtain their assets only after they get accessed. An SSG thus minimizes the computational demand on the server, which allows providers, such as GitHub Pages and others\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e, to host your static sites for free. More importantly, SSGs emanate a philosophy of simplicity-by-design stemming from a number of limitations. Being constraint forces content creators to focus on the content itself and minimizes the distractions of layout and design. SSGs are thus especially suited for blogs, landing pages, documentation and portfolio sites.\u003c/p\u003e","title":"How To Create This Blog"},{"content":"Data Artifacts An anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive 1. These anti-patterns can be identified in management approaches, software design, programming, and probably whenever people come together to achieve anything.\nData Science has its own set of anti-patterns that data scientist and their project managers should be aware of. ‚ÄúProliferating Data Artifacts‚Äù refers to the behaviour of generating datasets which represent various stages of processing. The data scientists often have to wrangling the data into the right shape for analysis. This might include basic things like joining, filtering, mapping and aggregating data as well as any number of complex feature engineering techniques. There is a strong incentive to save the processed data after each processing step to avoid unnecessary re-processing when changes to the code affect only later steps.\nThe choice to save intermediate data artifacts can, however, become very costly when multiple versions of the same data exist. We all know this from trying to save word documents as _v1.doc, _v2.doc, etc. A colleague might email back some changes, but renames the document to _v2_John.doc destroying the whole idea of a well-defined genealogy of documents. God forbid anyone ever tries to save a document as _final.doc. Future versions with an ever-growing _final_final_... suffix are all but inevitable.\nUnlike text documents, datasets can easily take up gigabytes of space. Multiplied with the number of distinct processing steps and their variations this can grow rapidly to a level where the dreaded \u0026lsquo;out-of-space\u0026rsquo; warning hits the poor data scientist. According to Murphy\u0026rsquo;s law this will always happen just before a critical deadline. In any case, the data scientist often finds it hard to decide which dataset to delete, as he/she might be unsure which one was the latest or if it is still needed. As a consequence a disproportionate amount of time is spent on managing intermediate data artifacts and storage space.\nThe \u0026ldquo;Proliferating Data Artifacts\u0026rdquo; anti-pattern leads to the unnecessary management of intermediate data artifacts and storage space.\nPipelines to the Rescue But as with every anti-pattern ‚Äî there is a solution! Pipelines. Pipelines manage intermediate datasets under-the-hood. A pipeline is basically a directed acyclic graph that describes the step-wise processing of the data.\nIt is easy to see that any data artifact could be recreated by applying all the processing steps (arrows) that lead to it to the preceding data. All that is needed is the input data and a well-defined pipeline.\nThe pipeline approach has several advantages:\nThe user does not need to track, save, or pass on (large) data artifacts. Unnecessary re-processing of unchanged/unaffected data artifacts is avoided. A pipeline is defined in code and can be versioned. Data artifacts from any pipeline version can easily and reproducibly be re-created. There are many more advantages that a good pipeline implementation can offer, such as parallel processing of independent parts of the pipeline, being self-contained, restart after failure, and efficient propagation of data changes.\nSo there\u0026rsquo;s really no excuse not to use existing pipeline frameworks (or implement them yourself) and delete those proliferating data artifacts! 2\nWikipedia: Anti-Pattern\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe original version of this text was published 2018-05-09.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2021-11-16-data-science-anti-patterns/","summary":"\u003ch2 id=\"data-artifacts\"\u003eData Artifacts\u003c/h2\u003e\n\u003cp\u003eAn anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. These anti-patterns can be identified in management approaches, software design, programming, and probably whenever people come together to achieve anything.\u003c/p\u003e\n\u003cp\u003eData Science has its own set of anti-patterns that data scientist and their project managers should be aware of. ‚ÄúProliferating Data Artifacts‚Äù refers to the behaviour of generating datasets which represent various stages of processing. The data scientists often have to wrangling the data into the right shape for analysis. This might include basic things like joining, filtering, mapping and aggregating data as well as any number of complex feature engineering techniques. There is a strong incentive to save the processed data after each processing step to avoid unnecessary re-processing when changes to the code affect only later steps.\u003c/p\u003e","title":"Data Science Anti-Patterns: Proliferating Data Artifacts"},{"content":"1. Serve your model quickly. The ultimate measure of success is that the insights generated by your model are useful to someone. The sooner that someone looks at the model output the sooner he/she can provide valuable feedback. I\u0026rsquo;d not be surprised if currently the vast majority of advanced analytics projects are shelved or abandoned, because the predictions cannot be integrated into business processes in a meaningful way. Or something is predicted that was already known by other means. A quickly-served model will also help you answer all your questions around performance, visualization, and production environment. As a result you\u0026rsquo;re less likely to optimize prematurely, over-deliver, and you can ensure that the model can continue to exist past the end of the project.\n2. Data Science projects are inherently iterative. Data quality can usually not be determined before modelling and model experimentation might affect data acquisition and pre-processing. Going through these iterations is often more efficient than trying to ascertain truths about data quality and model choice in advance. Many clients are worried about the quality and quantity of their data and might want to delay the start of a data science project until they\u0026rsquo;re sure that success is guaranteed. However, the most efficient way to determine data quality is by modelling.\nData science projects are people projects.\n3. Data Science projects are similar to software projects. They benefit from team spirit, improved communication, and a culture that encourages experimentation and learning. Clients are often concerned that they\u0026rsquo;re not getting the ‚Äòbest‚Äô model and the ‚Äòmost accurate‚Äô prediction. But the fact that the speed and quality of delivery could be easily improved with a few simple measures is often forgotten. In the end, data science projects are people projects. Things like a distraction-free work environment, easy access to an adequate development environment, ergonomic workplace and hardware, and easy access to data, have a bigger impact on success than a particular model choice.\nBonus Rule: Chose technology and tooling to support all of the above.\nThree quick rules for successful Data Science projects. Interested readers might want to learn about a structured Data Science approach like CRISP-DM 1 next. 2\nCRoss Industry Standard Process for Data Mining (CRISP-DM)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe original version of this text was published 2018-02-08.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://www.talesofindustry.org/post/2021-08-03-three-rules-of-success/","summary":"\u003ch2 id=\"1-serve-your-model-quickly\"\u003e1. Serve your model quickly.\u003c/h2\u003e\n\u003cp\u003eThe ultimate measure of success is that the insights generated by your model are useful to someone. The sooner that someone looks at the model output the sooner he/she can provide valuable feedback. I\u0026rsquo;d not be surprised if currently the vast majority of advanced analytics projects are shelved or abandoned, because the predictions cannot be integrated into business processes in a meaningful way. Or something is predicted that was already known by other means. A quickly-served model will also help you answer all your questions around performance, visualization, and production environment. As a result you\u0026rsquo;re less likely to optimize prematurely, over-deliver, and you can ensure that the model can continue to exist past the end of the project.\u003c/p\u003e","title":"Three Rules of Success for Data Science Projects"}]